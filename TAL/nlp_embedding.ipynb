{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP 2 : Neural Embeddings, Text Classification, Text Generation\n",
    "\n",
    "\n",
    "To use statistical classifiers with text, it is first necessary to vectorize the text. In the first practical session we explored the **bag of word** model. \n",
    "\n",
    "Modern **state of the art** methods uses  embeddings to vectorize the text before classification in order to avoid feature engineering.\n",
    "\n",
    "## Dataset\n",
    "https://github.com/cedias/practicalNLP/tree/master/dataset\n",
    "\n",
    "## \"Modern\" NLP pipeline\n",
    "\n",
    "By opposition to the **bag of word** model, in the modern NLP pipeline everything is **embeddings**. Instead of encoding a text as a **sparse vector** of length $D$ (size of feature dictionnary) the goal is to encode the text in a meaningful dense vector of a small size $|e| <<< |D|$. \n",
    "\n",
    "\n",
    "The raw classification pipeline is then the following:\n",
    "\n",
    "```\n",
    "raw text ---|embedding table|-->  vectors --|Neural Net|--> class \n",
    "```\n",
    "\n",
    "\n",
    "### Using a  language model:\n",
    "\n",
    "How to tokenize the text and extract a feature dictionnary is still a manual task. To directly have meaningful embeddings, it is common to use a pre-trained language model such as `word2vec` which we explore in this practical.\n",
    "\n",
    "In this setting, the pipeline becomes the following:\n",
    "```\n",
    "      \n",
    "raw text ---|(pre-trained) Language Model|--> vectors --|classifier (or fine-tuning)|--> class \n",
    "```\n",
    "\n",
    "\n",
    "- #### Classic word embeddings\n",
    "\n",
    " - [Word2Vec](https://arxiv.org/abs/1301.3781)\n",
    " - [Glove](https://nlp.stanford.edu/projects/glove/)\n",
    "\n",
    "\n",
    "- #### bleeding edge language models techniques (only here for reference)\n",
    "\n",
    " - [UMLFIT](https://arxiv.org/abs/1801.06146)\n",
    " - [ELMO](https://arxiv.org/abs/1802.05365)\n",
    " - [GPT](https://blog.openai.com/language-unsupervised/)\n",
    " - [BERT](https://arxiv.org/abs/1810.04805)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Goal of this session:\n",
    "\n",
    "1. Train word embeddings on training dataset\n",
    "2. Tinker with the learnt embeddings and see learnt relations\n",
    "3. Tinker with pre-trained embeddings.\n",
    "4. Use those embeddings for classification\n",
    "5. Compare different embedding models\n",
    "6. Pytorch first look: learn to generate text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##  Loading data (same as in nlp 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "[\"The undoubted highlight of this movie is Peter O'Toole's performance. In turn wildly comical and terribly terribly tragic. Does anybody do it better than O'Toole? I don't think so. What a great face that man has!<br /><br />The story is an odd one and quite disturbing and emotionally intense in parts (especially toward the end) but it is also oddly touching and does succeed on many levels. However, I felt the film basically revolved around Peter O'Toole's luminous performance and I'm sure I wouldn't have enjoyed it even half as much if he hadn't been in it.\", 1]\n",
      "\n",
      "Number of test reviews :  25000\n",
      "----> # of positive :  12500\n",
      "----> # of negative :  12500\n",
      "\n",
      "['Although credit should have been given to Dr. Seuess for stealing the story-line of \"Horton Hatches The Egg\", this was a fine film. It touched both the emotions and the intellect. Due especially to the incredible performance of seven year old Justin Henry and a script that was sympathetic to each character (and each one\\'s predicament), the thought provoking elements linger long after the tear jerking ones are over. Overall, superior acting from a solid cast, excellent directing, and a very powerful script. The right touches of humor throughout help keep a \"heavy\" subject from becoming tedious or difficult to sit through. Lastly, this film stands the test of time and seems in no way dated, decades after it was released.', 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "#### /!\\ YOU NEED TO UNZIP dataset/json_pol.zip first /!\\\n",
    "\n",
    "\n",
    "# Loading json\n",
    "with open(\"dataset/json_pol\",encoding=\"utf-8\") as f:\n",
    "    data = f.readlines()\n",
    "    json_data = json.loads(data[0])\n",
    "    train = json_data[\"train\"]\n",
    "    test = json_data[\"test\"]\n",
    "    \n",
    "\n",
    "# Quick Check\n",
    "counter_train = Counter((x[1] for x in train))\n",
    "counter_test = Counter((x[1] for x in test))\n",
    "print(\"Number of train reviews : \", len(train))\n",
    "print(\"----> # of positive : \", counter_train[1])\n",
    "print(\"----> # of negative : \", counter_train[0])\n",
    "print(\"\")\n",
    "print(train[0])\n",
    "print(\"\")\n",
    "print(\"Number of test reviews : \",len(test))\n",
    "print(\"----> # of positive : \", counter_test[1])\n",
    "print(\"----> # of negative : \", counter_test[0])\n",
    "\n",
    "print(\"\")\n",
    "print(test[0])\n",
    "print(\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec: Quick Recap\n",
    "\n",
    "**[Word2Vec](https://arxiv.org/abs/1301.3781) is composed of two distinct language models (CBOW and SG), optimized to quickly learn word vectors**\n",
    "\n",
    "\n",
    "given a random text: `i'm taking the dog out for a walk`\n",
    "\n",
    "\n",
    "\n",
    "### (a) Continuous Bag of Word (CBOW)\n",
    "    -  predicts a word given a context\n",
    "    \n",
    "maximizing `p(dog | i'm taking the ___ out for a walk)`\n",
    "    \n",
    "### (b) Skip-Gram (SG)               \n",
    "    -  predicts a context given a word\n",
    "    \n",
    " maximizing `p(i'm taking the out for a walk | dog)`\n",
    "\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: train (or load) a language model (word2vec)\n",
    "\n",
    "Gensim has one of [Word2Vec](https://radimrehurek.com/gensim/models/word2vec.html) fastest implementation.\n",
    "\n",
    "\n",
    "### Train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-22 12:27:23,900 : INFO : collecting all words and their counts\n",
      "2021-05-22 12:27:23,901 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-05-22 12:27:24,482 : INFO : PROGRESS: at sentence #10000, processed 2358544 words, keeping 140969 word types\n",
      "2021-05-22 12:27:25,069 : INFO : PROGRESS: at sentence #20000, processed 4675912 words, keeping 218529 word types\n",
      "2021-05-22 12:27:25,364 : INFO : collected 251637 word types from a corpus of 5844680 raw words and 25000 sentences\n",
      "2021-05-22 12:27:25,365 : INFO : Loading a fresh vocabulary\n",
      "2021-05-22 12:27:25,843 : INFO : effective_min_count=5 retains 46157 unique words (18% of original 251637, drops 205480)\n",
      "2021-05-22 12:27:25,844 : INFO : effective_min_count=5 leaves 5552884 word corpus (95% of original 5844680, drops 291796)\n",
      "2021-05-22 12:27:26,026 : INFO : deleting the raw counts dictionary of 251637 items\n",
      "2021-05-22 12:27:26,032 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2021-05-22 12:27:26,033 : INFO : downsampling leaves estimated 4214026 word corpus (75.9% of prior 5552884)\n",
      "2021-05-22 12:27:26,217 : INFO : estimated required memory for 46157 words and 100 dimensions: 60004100 bytes\n",
      "2021-05-22 12:27:26,218 : INFO : resetting layer weights\n",
      "2021-05-22 12:27:38,383 : INFO : training model with 3 workers on 46157 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-05-22 12:27:39,401 : INFO : EPOCH 1 - PROGRESS: at 17.23% examples, 728691 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:40,404 : INFO : EPOCH 1 - PROGRESS: at 35.50% examples, 751403 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:41,405 : INFO : EPOCH 1 - PROGRESS: at 57.17% examples, 804949 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:42,407 : INFO : EPOCH 1 - PROGRESS: at 71.94% examples, 756154 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:43,409 : INFO : EPOCH 1 - PROGRESS: at 87.68% examples, 737545 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:43,988 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-22 12:27:43,992 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-22 12:27:44,003 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-22 12:27:44,004 : INFO : EPOCH - 1 : training on 5844680 raw words (4214356 effective words) took 5.6s, 751244 effective words/s\n",
      "2021-05-22 12:27:45,012 : INFO : EPOCH 2 - PROGRESS: at 20.76% examples, 877755 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:46,012 : INFO : EPOCH 2 - PROGRESS: at 40.88% examples, 863086 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:47,018 : INFO : EPOCH 2 - PROGRESS: at 62.74% examples, 884871 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:48,027 : INFO : EPOCH 2 - PROGRESS: at 85.83% examples, 900583 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:48,623 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-22 12:27:48,625 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-22 12:27:48,634 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-22 12:27:48,635 : INFO : EPOCH - 2 : training on 5844680 raw words (4213358 effective words) took 4.6s, 910682 effective words/s\n",
      "2021-05-22 12:27:49,648 : INFO : EPOCH 3 - PROGRESS: at 18.12% examples, 764981 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:50,650 : INFO : EPOCH 3 - PROGRESS: at 36.18% examples, 766507 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:51,663 : INFO : EPOCH 3 - PROGRESS: at 56.35% examples, 790796 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:52,676 : INFO : EPOCH 3 - PROGRESS: at 80.23% examples, 837293 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:53,660 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-22 12:27:53,662 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-22 12:27:53,671 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-22 12:27:53,671 : INFO : EPOCH - 3 : training on 5844680 raw words (4215316 effective words) took 5.0s, 838233 effective words/s\n",
      "2021-05-22 12:27:54,683 : INFO : EPOCH 4 - PROGRESS: at 15.20% examples, 644018 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:55,683 : INFO : EPOCH 4 - PROGRESS: at 33.04% examples, 698853 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:56,698 : INFO : EPOCH 4 - PROGRESS: at 50.00% examples, 703333 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:57,704 : INFO : EPOCH 4 - PROGRESS: at 71.04% examples, 744207 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:58,710 : INFO : EPOCH 4 - PROGRESS: at 88.87% examples, 744422 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:27:59,331 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-22 12:27:59,340 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-22 12:27:59,351 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-22 12:27:59,351 : INFO : EPOCH - 4 : training on 5844680 raw words (4214757 effective words) took 5.7s, 742409 effective words/s\n",
      "2021-05-22 12:28:00,355 : INFO : EPOCH 5 - PROGRESS: at 21.09% examples, 893980 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:28:01,357 : INFO : EPOCH 5 - PROGRESS: at 43.66% examples, 923455 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:28:02,361 : INFO : EPOCH 5 - PROGRESS: at 60.36% examples, 850581 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:28:03,366 : INFO : EPOCH 5 - PROGRESS: at 81.07% examples, 850464 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-22 12:28:04,333 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-22 12:28:04,339 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-22 12:28:04,341 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-22 12:28:04,342 : INFO : EPOCH - 5 : training on 5844680 raw words (4213755 effective words) took 5.0s, 844927 effective words/s\n",
      "2021-05-22 12:28:04,343 : INFO : training on a 29223400 raw words (21071542 effective words) took 26.0s, 811767 effective words/s\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "text = [t.lower().split() for t,p in train]\n",
    "\n",
    "# the following configuration is the default configuration\n",
    "w2v = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=0, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1,\n",
    "                                iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blood,', 0.7728791236877441), ('violence', 0.6947540044784546), ('gore,', 0.6719174385070801), ('nudity', 0.6580226421356201), ('gratuitous', 0.6537489891052246)]\n"
     ]
    }
   ],
   "source": [
    "#most similar\n",
    "print(w2v.wv.most_similar(['blood'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gun', 0.008440515), ('machine', 0.002059182), ('opens', 0.001786327), ('shoots', 0.0017689086), ('fight', 0.0016783742)]\n"
     ]
    }
   ],
   "source": [
    "#CBOW\n",
    "print(w2v.predict_output_word(['gun'],topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-01 23:37:18,591 : INFO : collecting all words and their counts\n",
      "2021-05-01 23:37:18,592 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-05-01 23:37:19,033 : INFO : PROGRESS: at sentence #10000, processed 2358544 words, keeping 140969 word types\n",
      "2021-05-01 23:37:19,493 : INFO : PROGRESS: at sentence #20000, processed 4675912 words, keeping 218529 word types\n",
      "2021-05-01 23:37:19,749 : INFO : collected 251637 word types from a corpus of 5844680 raw words and 25000 sentences\n",
      "2021-05-01 23:37:19,750 : INFO : Loading a fresh vocabulary\n",
      "2021-05-01 23:37:20,052 : INFO : effective_min_count=5 retains 46157 unique words (18% of original 251637, drops 205480)\n",
      "2021-05-01 23:37:20,053 : INFO : effective_min_count=5 leaves 5552884 word corpus (95% of original 5844680, drops 291796)\n",
      "2021-05-01 23:37:20,211 : INFO : deleting the raw counts dictionary of 251637 items\n",
      "2021-05-01 23:37:20,217 : INFO : sample=0.001 downsamples 45 most-common words\n",
      "2021-05-01 23:37:20,218 : INFO : downsampling leaves estimated 4214026 word corpus (75.9% of prior 5552884)\n",
      "2021-05-01 23:37:20,369 : INFO : estimated required memory for 46157 words and 100 dimensions: 60004100 bytes\n",
      "2021-05-01 23:37:20,371 : INFO : resetting layer weights\n",
      "2021-05-01 23:37:30,221 : INFO : training model with 3 workers on 46157 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-05-01 23:37:31,228 : INFO : EPOCH 1 - PROGRESS: at 7.10% examples, 308189 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:32,231 : INFO : EPOCH 1 - PROGRESS: at 14.89% examples, 316042 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:33,237 : INFO : EPOCH 1 - PROGRESS: at 22.28% examples, 313313 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:34,240 : INFO : EPOCH 1 - PROGRESS: at 29.80% examples, 314492 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:35,249 : INFO : EPOCH 1 - PROGRESS: at 37.20% examples, 314563 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:36,264 : INFO : EPOCH 1 - PROGRESS: at 44.63% examples, 313176 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:37,277 : INFO : EPOCH 1 - PROGRESS: at 52.19% examples, 314515 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:38,295 : INFO : EPOCH 1 - PROGRESS: at 59.89% examples, 314272 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:39,306 : INFO : EPOCH 1 - PROGRESS: at 67.55% examples, 314386 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:40,337 : INFO : EPOCH 1 - PROGRESS: at 75.48% examples, 314701 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:41,366 : INFO : EPOCH 1 - PROGRESS: at 83.21% examples, 314586 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:42,376 : INFO : EPOCH 1 - PROGRESS: at 90.68% examples, 314862 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:43,381 : INFO : EPOCH 1 - PROGRESS: at 98.36% examples, 315189 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:43,555 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-01 23:37:43,573 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-01 23:37:43,587 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-01 23:37:43,588 : INFO : EPOCH - 1 : training on 5844680 raw words (4213962 effective words) took 13.4s, 315328 effective words/s\n",
      "2021-05-01 23:37:44,602 : INFO : EPOCH 2 - PROGRESS: at 7.10% examples, 306629 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:45,607 : INFO : EPOCH 2 - PROGRESS: at 14.89% examples, 315007 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:46,630 : INFO : EPOCH 2 - PROGRESS: at 22.61% examples, 315373 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:47,635 : INFO : EPOCH 2 - PROGRESS: at 30.13% examples, 315928 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:48,665 : INFO : EPOCH 2 - PROGRESS: at 37.72% examples, 315763 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:49,670 : INFO : EPOCH 2 - PROGRESS: at 45.39% examples, 317095 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:50,670 : INFO : EPOCH 2 - PROGRESS: at 52.76% examples, 316417 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:51,695 : INFO : EPOCH 2 - PROGRESS: at 60.36% examples, 315665 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:52,716 : INFO : EPOCH 2 - PROGRESS: at 68.04% examples, 315286 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:53,733 : INFO : EPOCH 2 - PROGRESS: at 76.00% examples, 315989 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:54,734 : INFO : EPOCH 2 - PROGRESS: at 83.52% examples, 315939 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:55,735 : INFO : EPOCH 2 - PROGRESS: at 90.84% examples, 315724 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:56,736 : INFO : EPOCH 2 - PROGRESS: at 98.55% examples, 316107 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:56,888 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-01 23:37:56,890 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-01 23:37:56,929 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-01 23:37:56,929 : INFO : EPOCH - 2 : training on 5844680 raw words (4214912 effective words) took 13.3s, 316031 effective words/s\n",
      "2021-05-01 23:37:57,935 : INFO : EPOCH 3 - PROGRESS: at 7.10% examples, 309282 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:58,935 : INFO : EPOCH 3 - PROGRESS: at 14.89% examples, 316672 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:37:59,953 : INFO : EPOCH 3 - PROGRESS: at 22.61% examples, 317359 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:00,956 : INFO : EPOCH 3 - PROGRESS: at 30.13% examples, 317395 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:02,001 : INFO : EPOCH 3 - PROGRESS: at 37.72% examples, 316033 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:03,018 : INFO : EPOCH 3 - PROGRESS: at 45.58% examples, 317874 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:04,073 : INFO : EPOCH 3 - PROGRESS: at 53.28% examples, 316596 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:05,089 : INFO : EPOCH 3 - PROGRESS: at 60.94% examples, 317005 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:06,099 : INFO : EPOCH 3 - PROGRESS: at 68.92% examples, 317718 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:07,108 : INFO : EPOCH 3 - PROGRESS: at 76.66% examples, 317785 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:08,112 : INFO : EPOCH 3 - PROGRESS: at 84.01% examples, 316786 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:09,114 : INFO : EPOCH 3 - PROGRESS: at 91.50% examples, 317059 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:10,121 : INFO : EPOCH 3 - PROGRESS: at 99.26% examples, 317185 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:10,192 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-01 23:38:10,215 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-01 23:38:10,227 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-01 23:38:10,227 : INFO : EPOCH - 3 : training on 5844680 raw words (4214719 effective words) took 13.3s, 317007 effective words/s\n",
      "2021-05-01 23:38:11,239 : INFO : EPOCH 4 - PROGRESS: at 6.94% examples, 300171 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:12,241 : INFO : EPOCH 4 - PROGRESS: at 14.72% examples, 311876 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:13,257 : INFO : EPOCH 4 - PROGRESS: at 22.48% examples, 314166 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:14,270 : INFO : EPOCH 4 - PROGRESS: at 30.13% examples, 316057 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:15,297 : INFO : EPOCH 4 - PROGRESS: at 37.72% examples, 316032 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:16,308 : INFO : EPOCH 4 - PROGRESS: at 45.23% examples, 315910 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:17,316 : INFO : EPOCH 4 - PROGRESS: at 52.76% examples, 315997 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:18,322 : INFO : EPOCH 4 - PROGRESS: at 60.36% examples, 316034 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:19,335 : INFO : EPOCH 4 - PROGRESS: at 68.21% examples, 316677 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:20,349 : INFO : EPOCH 4 - PROGRESS: at 76.00% examples, 316680 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:21,362 : INFO : EPOCH 4 - PROGRESS: at 83.70% examples, 316814 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-01 23:38:22,373 : INFO : EPOCH 4 - PROGRESS: at 91.18% examples, 316817 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:23,383 : INFO : EPOCH 4 - PROGRESS: at 98.91% examples, 316904 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:23,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-01 23:38:23,521 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-01 23:38:23,527 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-01 23:38:23,528 : INFO : EPOCH - 4 : training on 5844680 raw words (4213905 effective words) took 13.3s, 316883 effective words/s\n",
      "2021-05-01 23:38:24,556 : INFO : EPOCH 5 - PROGRESS: at 6.94% examples, 295525 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:25,572 : INFO : EPOCH 5 - PROGRESS: at 14.72% examples, 307530 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:26,572 : INFO : EPOCH 5 - PROGRESS: at 22.28% examples, 310381 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:27,593 : INFO : EPOCH 5 - PROGRESS: at 29.80% examples, 310931 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:28,619 : INFO : EPOCH 5 - PROGRESS: at 37.39% examples, 312021 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:29,628 : INFO : EPOCH 5 - PROGRESS: at 45.08% examples, 313733 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:30,653 : INFO : EPOCH 5 - PROGRESS: at 52.76% examples, 314392 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:31,668 : INFO : EPOCH 5 - PROGRESS: at 60.50% examples, 315149 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:32,675 : INFO : EPOCH 5 - PROGRESS: at 68.21% examples, 315356 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:33,695 : INFO : EPOCH 5 - PROGRESS: at 76.00% examples, 315280 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:34,713 : INFO : EPOCH 5 - PROGRESS: at 83.70% examples, 315377 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:35,723 : INFO : EPOCH 5 - PROGRESS: at 91.18% examples, 315554 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:36,736 : INFO : EPOCH 5 - PROGRESS: at 99.11% examples, 316205 words/s, in_qsize 5, out_qsize 0\n",
      "2021-05-01 23:38:36,823 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-05-01 23:38:36,824 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-05-01 23:38:36,863 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-05-01 23:38:36,863 : INFO : EPOCH - 5 : training on 5844680 raw words (4213634 effective words) took 13.3s, 316059 effective words/s\n",
      "2021-05-01 23:38:36,864 : INFO : training on a 29223400 raw words (21071132 effective words) took 66.6s, 316183 effective words/s\n"
     ]
    }
   ],
   "source": [
    "w2v_sg = gensim.models.word2vec.Word2Vec(sentences=text,\n",
    "                                size=100, window=5,               ### here we train a cbow model \n",
    "                                min_count=5,                      \n",
    "                                sample=0.001, workers=3,\n",
    "                                sg=1, hs=0, negative=5,        ### set sg to 1 to train a sg model\n",
    "                                cbow_mean=1,\n",
    "                                iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('blood,', 0.7791843414306641), ('gore', 0.7405943870544434), ('bloody', 0.7211947441101074), ('blood.', 0.6931116580963135), ('buckets', 0.6863688230514526)]\n"
     ]
    }
   ],
   "source": [
    "#most similar\n",
    "print(w2v_sg.wv.most_similar(['blood'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gun', 0.005693438), ('machine', 0.0013765082), ('shoots', 0.00090895087), ('arm', 0.00081521034), ('guns', 0.0006648248)]\n"
     ]
    }
   ],
   "source": [
    "#SG\n",
    "print(w2v_sg.predict_output_word(['gun'],topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load pre-trained embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-05-02 00:21:39,727 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-05-02 00:21:39,729 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2021-05-02 00:21:39,730 : INFO : loading projection weights from C:\\Users\\William\\Anaconda3\\lib\\site-packages\\gensim\\test\\test_data\\word2vec_pre_kv_c\n",
      "2021-05-02 00:21:39,775 : INFO : loaded (1750, 10) matrix from C:\\Users\\William\\Anaconda3\\lib\\site-packages\\gensim\\test\\test_data\\word2vec_pre_kv_c\n"
     ]
    }
   ],
   "source": [
    "# It's for later\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "wv_from_text = KeyedVectors.load_word2vec_format(datapath('word2vec_pre_kv_c'), binary=False)  # C text format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Gensim, embeddings are loaded and can be used via the [\"KeyedVectors\"](https://radimrehurek.com/gensim/models/keyedvectors.html) class\n",
    "\n",
    "> Since trained word vectors are independent from the way they were trained (Word2Vec, FastText, WordRank, VarEmbed etc), they can be represented by a standalone structure, as implemented in this module.\n",
    "\n",
    ">The structure is called “KeyedVectors” and is essentially a mapping between entities and vectors. Each entity is identified by its string id, so this is a mapping between {str => 1D numpy array}.\n",
    "\n",
    ">The entity typically corresponds to a word (so the mapping maps words to 1D vectors), but for some models, they key can also correspond to a document, a graph node etc. To generalize over different use-cases, this module calls the keys entities. Each entity is always represented by its string id, no matter whether the entity is a word, a document or a graph node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Test learnt embeddings\n",
    "\n",
    "The word embedding space directly encodes similarities between words: the vector coding for the word \"great\" will be closer to the vector coding for \"good\" than to the one coding for \"bad\". Generally, [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) is the distance used when considering distance between vectors.\n",
    "\n",
    "KeyedVectors have a built in [similarity](https://radimrehurek.com/gensim/models /keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.similarity) method to compute the cosine similarity between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle CBOW : great and good: 0.7663591\n",
      "Modèle CBOW : great and bad: 0.5138367\n",
      "Modèle SG : great and good: 0.7585448\n",
      "Modèle SG : great and bad: 0.47639462\n"
     ]
    }
   ],
   "source": [
    "# is great really closer to good than to bad ?\n",
    "print(\"Modèle CBOW : great and good:\",w2v.wv.similarity(\"great\",\"good\"))\n",
    "print(\"Modèle CBOW : great and bad:\",w2v.wv.similarity(\"great\",\"bad\"))\n",
    "print(\"Modèle SG : great and good:\",w2v_sg.wv.similarity(\"great\",\"good\"))\n",
    "print(\"Modèle SG : great and bad:\",w2v_sg.wv.similarity(\"great\",\"bad\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since cosine distance encodes similarity, neighboring words are supposed to be similar. The [most_similar](https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.BaseKeyedVectors.most_similar) method returns the `topn` words given a query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('digital', 0.6952984929084778), ('technology', 0.6548064947128296), ('make-up', 0.6426851749420166), ('cg', 0.6425226926803589), ('camera,', 0.6424719095230103)]\n"
     ]
    }
   ],
   "source": [
    "# The query can be as simple as a word, such as \"movie\"\n",
    "\n",
    "# Try changing the word\n",
    "#print(w2v.wv.most_similar(\"movie\",topn=5)) # 5 most similar words\n",
    "# print(w2v.wv.most_similar(\"awesome\",topn=5))\n",
    "print(w2v.wv.most_similar(\"computer\",topn=5))\n",
    "# print(w2v_sg.wv.most_similar(\"movie\",topn=5)) # 5 most similar words\n",
    "# print(w2v_sg.wv.most_similar(\"awesome\",topn=5))\n",
    "# print(w2v_sg.wv.most_similar(\"actor\",topn=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it can be a more complicated query\n",
    "Word embedding spaces tend to encode much more.\n",
    "\n",
    "The most famous exemple is: `vec(king) - vec(man) + vec(woman) => vec(queen)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('gardener', 0.7268567681312561), ('dealer', 0.7106071710586548), ('nerdy', 0.7070906162261963)]\n"
     ]
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "#print(w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3))\n",
    "\n",
    "#print(w2v.wv.most_similar(positive=[\"man\",\"woman\"],negative=[\"king\"],topn=5)) # do the famous exemple works for actor ?\n",
    "#print(w2v.wv.most_similar(positive=[\"data\",\"student\",\"science\"],negative=[\"code\"],topn=3))\n",
    "# Try other things like plurals for exemple.\n",
    "#print(w2v.wv.most_similar(positive=[\"women\",\"men\"],negative=[\"man\"],topn=3))\n",
    "\n",
    "print(w2v.wv.most_similar(positive=[\"data\",\"student\"],negative=[\"code\"],topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pianist', 0.7420253157615662), ('mechanic', 0.7139220833778381), ('adores', 0.7137610912322998)]\n",
      "[('novels', 0.7172544002532959), ('adaptations', 0.6937443017959595), ('bloch', 0.6852920055389404)]\n"
     ]
    }
   ],
   "source": [
    "# What is awesome - good + bad ?\n",
    "#print(w2v.wv.most_similar(positive=[\"awesome\",\"bad\"],negative=[\"good\"],topn=3))\n",
    "\n",
    "#print(w2v.wv.most_similar(positive=[\"man\",\"woman\"],negative=[\"king\"],topn=5)) # do the famous exemple works for actor ?\n",
    "print(w2v.wv.most_similar(positive=[\"data\",\"student\"],negative=[\"code\"],topn=3))\n",
    "#print(w2v.wv.most_similar(positive=[\"data\",\"student\",\"science\"],negative=[\"code\"],topn=3))\n",
    "# Try other things like plurals for exemple.\n",
    "print(w2v.wv.most_similar(positive=[\"beds\",\"books\"],negative=[\"sleep\"],topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test learnt \"synctactic\" and \"semantic\" similarities, Mikolov et al. introduced a special dataset containing a wide variety of three way similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `accuracy` (Method will be removed in 4.0.0, use self.evaluate_word_analogies() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "2021-04-29 17:37:30,480 : INFO : capital-common-countries: 3.6% (2/56)\n",
      "2021-04-29 17:37:30,571 : INFO : capital-world: 3.7% (1/27)\n",
      "2021-04-29 17:37:30,631 : INFO : currency: 0.0% (0/18)\n",
      "2021-04-29 17:37:31,379 : INFO : city-in-state: 0.4% (1/249)\n",
      "2021-04-29 17:37:32,398 : INFO : family: 36.5% (125/342)\n",
      "2021-04-29 17:37:35,387 : INFO : gram1-adjective-to-adverb: 0.8% (7/870)\n",
      "2021-04-29 17:37:36,771 : INFO : gram2-opposite: 2.4% (11/462)\n",
      "2021-04-29 17:37:39,912 : INFO : gram3-comparative: 22.8% (241/1056)\n",
      "2021-04-29 17:37:41,598 : INFO : gram4-superlative: 11.1% (61/552)\n",
      "2021-04-29 17:37:43,867 : INFO : gram5-present-participle: 16.1% (122/756)\n",
      "2021-04-29 17:37:46,036 : INFO : gram6-nationality-adjective: 0.9% (7/737)\n",
      "2021-04-29 17:37:49,647 : INFO : gram7-past-tense: 18.4% (219/1190)\n",
      "2021-04-29 17:37:51,892 : INFO : gram8-plural: 5.2% (39/756)\n",
      "2021-04-29 17:37:53,526 : INFO : gram9-plural-verbs: 19.6% (108/552)\n",
      "2021-04-29 17:37:53,526 : INFO : total: 12.4% (944/7623)\n"
     ]
    }
   ],
   "source": [
    "out = w2v.wv.accuracy(\"dataset/questions-words.txt\",case_insensitive=True)  #original semantic syntactic dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training the w2v models on the review dataset, since it hasn't been learnt with a lot of data, it does not perform very well. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3:  sentiment classification\n",
    "\n",
    "In the previous practical session, we used a bag of word approach to transform text into vectors.\n",
    "Here, we propose to try to use word vectors (previously learnt or loaded).\n",
    "\n",
    "\n",
    "### <font color='green'> Since we have only word vectors and that sentences are made of multiple words, we need to aggregate them. </font>\n",
    "\n",
    "\n",
    "### (1) Vectorize reviews using word vectors:\n",
    "\n",
    "Word aggregation can be done in different ways:\n",
    "\n",
    "- Sum\n",
    "- Average\n",
    "- Min/feature\n",
    "- Max/feature\n",
    "\n",
    "#### a few pointers:\n",
    "\n",
    "- `w2v.wv.vocab` is a `set()` of the vocabulary (all existing words in your model)\n",
    "- `np.minimum(a,b) and np.maximum(a,b)` respectively return element-wise min/max "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "# We first need to vectorize text:\n",
    "# First we propose to a sum of them\n",
    "\n",
    "def vectorize(text, model, mean=False):\n",
    "    \"\"\"\n",
    "    This function should vectorize one review\n",
    "    \"\"\"\n",
    "    text = text.split()\n",
    "    vec = np.zeros(model.vector_size)\n",
    "    cpt = 0\n",
    "    for word in text:\n",
    "        if word in model.vocab:\n",
    "            vec += model.get_vector(word)\n",
    "            cpt += 1\n",
    "    if (mean == True) and (cpt != 0):\n",
    "        vec /= cpt\n",
    "\n",
    "    return vec\n",
    "\n",
    "# classes = [pol for text,pol in train]\n",
    "# X = [vectorize(text) for text,pol in train]\n",
    "# X_test = [vectorize(text) for text,pol in test]\n",
    "# true = [pol for text,pol in test]\n",
    "\n",
    "def evalutation(models, models_name):\n",
    "    res = []\n",
    "    logreg = LogisticRegression(random_state=0)\n",
    "    classes = [pol for text,pol in train]\n",
    "    true = [pol for text,pol in test]\n",
    "    for mi in range(len(models)):\n",
    "        X = [vectorize(text, models[mi]) for text,pol in train]\n",
    "        X_test = [vectorize(text, models[mi]) for text,pol in test]\n",
    "        logreg.fit(X,classes)\n",
    "        ypred = logreg.predict(X_test)\n",
    "        res.append({'Model':models_name[mi],'Aggregation':'sum','Score':accuracy_score(true, ypred)})\n",
    "        \n",
    "        X_mean = [vectorize(text, models[mi], True) for text,pol in train]\n",
    "        X_test_mean = [vectorize(text, models[mi], True) for text,pol in test]\n",
    "        logreg.fit(X_mean,classes)\n",
    "        ypred = logreg.predict(X_test_mean)\n",
    "        accuracy_score(true, ypred)\n",
    "        res.append({'Model':models_name[mi],'Aggregation':'mean','Score':accuracy_score(true, ypred)})\n",
    "        \n",
    "    return res \n",
    "#let's see what a review vector looks like.\n",
    "#print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Train a classifier \n",
    "as in the previous practical session, train a logistic regression to do sentiment classification with word vectors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(w2v.wv.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "C:\\Users\\William\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 8s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Aggregation</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>w2v_sg</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.82160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>w2v_sg</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.82108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>w2v_cbow</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.77892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>w2v_cbow</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.77764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>w2v_pretrained</td>\n",
       "      <td>sum</td>\n",
       "      <td>0.58000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>w2v_pretrained</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.56612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model Aggregation    Score\n",
       "4          w2v_sg         sum  0.82160\n",
       "5          w2v_sg        mean  0.82108\n",
       "3        w2v_cbow        mean  0.77892\n",
       "2        w2v_cbow         sum  0.77764\n",
       "0  w2v_pretrained         sum  0.58000\n",
       "1  w2v_pretrained        mean  0.56612"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "# Scikit Logistic Regression\n",
    "# logreg = LogisticRegression()\n",
    "# logreg.fit(X,classes)\n",
    "# ypred = logreg.predict(X_test)\n",
    "\n",
    "# print(accuracy_score(true, ypred))\n",
    "\n",
    "models = [wv_from_text, w2v.wv, w2v_sg.wv]\n",
    "names = [\"w2v_pretrained\",\"w2v_cbow\", \"w2v_sg\"]\n",
    "eval = evalutation(models, names)\n",
    "pd.DataFrame(eval).sort_values(by='Score', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "performance should be worst than with bag of word (~80%). Sum/Mean aggregation does not work well on long reviews (especially with many frequent words). This adds a lot of noise.\n",
    "\n",
    "## **Todo** :  Try answering the following questions:\n",
    "\n",
    "- Which word2vec model works best: skip-gram or cbow\n",
    "- Do pretrained vectors work best than those learnt on the train dataset ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**(Bonus)** To have a better accuracy, we could try two things:\n",
    "- Better aggregation methods (weight by tf-idf ?)\n",
    "- Another word vectorizing method such as [fasttext](https://radimrehurek.com/gensim/models/fasttext.html)\n",
    "- A document vectorizing method such as [Doc2Vec](https://radimrehurek.com/gensim/models/doc2vec.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --- Generate text with a recurrent neural network (Pytorch) ---\n",
    "### (Mostly Read & Run)\n",
    "\n",
    "The goal is to replicate the (famous) experiment from [Karpathy's blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "To learn to generate text, we train a recurrent neural network to do the following task:\n",
    "\n",
    "Given a \"chunk\" of text: `this is random text`\n",
    "\n",
    "the goal of the network is to predict each character in **`his is random text` ** sequentially given the following sequential input **`this is random tex`**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Input ->  Output\n",
    "--------------\n",
    "T    ->    H\n",
    "H    ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "\" \"  ->    I\n",
    "I    ->    S\n",
    "S    ->    \" \"\n",
    "[...]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Load text (dataset/input.txt)\n",
    "\n",
    "Before building training batch, we load the full text in RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 1115394\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('dataset/input.txt').read()) #clean text => only ascii\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Helper functions:\n",
    "\n",
    "We have a text and we want to feed batch of chunks to a neural network:\n",
    "\n",
    "one chunk  A,B,C,D,E\n",
    "[input] A,B,C,D -> B,C,D,E [output]\n",
    "\n",
    "Note: we will use an embedding layer instead of a one-hot encoding scheme.\n",
    "\n",
    "for this, we have 3 functions:\n",
    "\n",
    "- One to get a random str chunk of size `chunk_len` : `random_chunk` \n",
    "- One to turn a chunk into a tensor of size `(1,chunk_len)` coding for each characters : `char_tensor`\n",
    "- One to return random input and output chunks of size `(batch_size,chunk_len)` : `random_training_set`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[28, 29, 94, 48, 30, 28, 18, 12, 18, 10],\n",
      "        [24, 27, 13, 73, 94, 32, 18, 29, 17, 94],\n",
      "        [11, 21, 14, 94, 15, 24, 27, 94, 10, 94],\n",
      "        [96, 46, 44, 49, 42, 94, 40, 39, 58, 36]]), tensor([[29, 94, 48, 30, 28, 18, 12, 18, 10, 23],\n",
      "        [27, 13, 73, 94, 32, 18, 29, 17, 94, 10],\n",
      "        [21, 14, 94, 15, 24, 27, 94, 10, 94, 23],\n",
      "        [46, 44, 49, 42, 94, 40, 39, 58, 36, 53]]))\n"
     ]
    }
   ],
   "source": [
    "import time, math\n",
    "\n",
    "\n",
    "#Get a piece of text\n",
    "def random_chunk(chunk_len):\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "\n",
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(1,len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[0,c] = all_characters.index(string[c])\n",
    "    return tensor\n",
    "\n",
    "\n",
    "#Turn a piece of text in train/test\n",
    "def random_training_set(chunk_len=200, batch_size=8):\n",
    "    chunks = [random_chunk(chunk_len) for _ in range(batch_size)]\n",
    "    inp = torch.cat([char_tensor(chunk[:-1]) for chunk in chunks],dim=0)\n",
    "    target = torch.cat([char_tensor(chunk[1:]) for chunk in chunks],dim=0)\n",
    "    \n",
    "    return inp, target\n",
    "\n",
    "print(random_training_set(10,4))  ## should return 8 chunks of 10 letters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The actual RNN model (only thing to complete):\n",
    "\n",
    "It should be composed of three distinct modules:\n",
    "\n",
    "- an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) (n_characters, hidden_size)\n",
    "\n",
    "```\n",
    "nn.Embedding(len_dic,size_vec)\n",
    "```\n",
    "- a [recurrent](https://pytorch.org/docs/stable/nn.html#recurrent-layers) layer (hidden_size, hidden_size)\n",
    "```\n",
    "nn.RNN(in_size,out_size) or nn.GRU() or nn.LSTM() => rnn_cell parameter\n",
    "```\n",
    "- a [prediction](https://pytorch.org/docs/stable/nn.html#linear) layer (hidden_size, output_size)\n",
    "\n",
    "```\n",
    "nn.Linear(in_size,out_size)\n",
    "```\n",
    "=> Complete the `init` function code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as f\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_char, hidden_size, output_size, n_layers=1,rnn_cell=nn.RNN):\n",
    "        \"\"\"\n",
    "        Create the network\n",
    "        \"\"\"\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.n_char = n_char\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        #  (batch,chunk_len) -> (batch, chunk_len, hidden_size)  \n",
    "        self.embed = nn.Embedding(n_char, hidden_size)\n",
    "        \n",
    "        # (batch, chunk_len, hidden_size)  -> (batch, chunk_len, hidden_size)  \n",
    "        self.rnn = rnn_cell(hidden_size, hidden_size)\n",
    "        \n",
    "        #(batch, chunk_len, hidden_size) -> (batch, chunk_len, output_size)  \n",
    "        self.predict = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        batched forward: input is (batch > 1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,_  = self.rnn(input)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output\n",
    "    \n",
    "    def forward_seq(self, input,hidden=None):\n",
    "        \"\"\"\n",
    "        not batched forward: input is  (1,chunk_len)\n",
    "        \"\"\"\n",
    "        input = self.embed(input)\n",
    "        output,hidden  = self.rnn(input.unsqueeze(0),hidden)\n",
    "        output = self.predict(f.tanh(output))\n",
    "        return output,hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Text generation function\n",
    "\n",
    "Sample text from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model,prime_str='A', predict_len=100, temperature=0.8):\n",
    "    prime_input = char_tensor(prime_str).squeeze(0)\n",
    "    hidden = None\n",
    "    predicted = prime_str+\"\"\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "\n",
    "    for p in range(len(prime_str)-1):\n",
    "        _,hidden = model.forward_seq(prime_input[p].unsqueeze(0),hidden)\n",
    "            \n",
    "    #print(hidden.size())\n",
    "    for p in range(predict_len):\n",
    "        output, hidden = model.forward_seq(prime_input[-1].unsqueeze(0), hidden)\n",
    "                # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        #print(output_dist)\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        #print(top_i)\n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        prime_input = torch.cat([prime_input,char_tensor(predicted_char).squeeze(0)])\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Training loop for net\n",
    "\n",
    "**RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 2s (100 1%) 2.5696]\n",
      "Whees blala; CARIf th lin\n",
      "I tanithare phiofanetinonechend I my cainch, g tlllew the athathater the y h \n",
      "\n",
      "[0m 5s (200 2%) 2.5100]\n",
      "Whiandeis te f ur:\n",
      "I:\n",
      "Wat in or G ngathr me's f man thes p, s inerounof KENowe thar y, must my at o q  \n",
      "\n",
      "[0m 8s (300 3%) 2.5400]\n",
      "Wher e f IULAnghis.\n",
      "\n",
      "; hare, t thinerellelir'd iey lder head whe thapor pouthesssthourir de r t s s br \n",
      "\n",
      "[0m 11s (400 4%) 2.4492]\n",
      "Whid d.\n",
      "Istowit ingh\n",
      "Anow' I I s:\n",
      "Tour be f ntithau LI a wond met dowe t te bin.\n",
      "\n",
      "S:\n",
      "Thea t seanas ito \n",
      "\n",
      "[0m 14s (500 5%) 2.5106]\n",
      "Where sy s Cid in: ay, my the thienkengel sethake e: s Gre we atasof se dousuy whewe tor melly mor t a \n",
      "\n",
      "[0m 16s (600 6%) 2.5494]\n",
      "Whoucoroo beay f inthenor r me:\n",
      "Thes, a s fom fanin.\n",
      "POns th.\n",
      "Whence, hesUCowent t sare l s t, heato i \n",
      "\n",
      "[0m 19s (700 7%) 2.4543]\n",
      "Whcelt s h fin t o r frethalarel CAngowinde ls Hands Loup'done t thain m sse?\n",
      "Myol d tar y t nt y ent  \n",
      "\n",
      "[0m 22s (800 8%) 2.5016]\n",
      "Wham onge o thasheat ch pe heatelicatistherifou the mun.\n",
      "Have I t s mofele wen and charanghe, 'ling im \n",
      "\n",
      "[0m 24s (900 9%) 2.4525]\n",
      "Whintheay wor thou I therve toupind,\n",
      "GENTAnd an cke suth beyovieanot sitin y hastan mau, hothouche tho \n",
      "\n",
      "[0m 27s (1000 10%) 2.4908]\n",
      "Whe nd co got ticistins: oon st ferer n hous t al mugond lereelind burd I avad strines t s he:\n",
      "Ane tie \n",
      "\n",
      "[0m 30s (1100 11%) 2.5001]\n",
      "Wher whany andsto ofre t tithy y heenourr t m ly\n",
      "SSadavere, me ue yourelarer nshin hanoould t d se\n",
      "Bun \n",
      "\n",
      "[0m 32s (1200 12%) 2.4803]\n",
      "Wham\n",
      "Sathe, k bra an tse.\n",
      "DWhe medinead ondint he amir s tureear tiant er thimedetheland heacho s ag t \n",
      "\n",
      "[0m 35s (1300 13%) 2.4071]\n",
      "Whe alen se ag fof?\n",
      "Boomut whe lo, henthrveat lly in t n\n",
      "Juros f otyoro s kesunomurt hiered,\n",
      "\n",
      "Mand s h \n",
      "\n",
      "[0m 38s (1400 14%) 2.4678]\n",
      "Wherasurd tr womer ast the as f Fou the ind toveand I hind ang ok by the ape f t momes hexpe he g woum \n",
      "\n",
      "[0m 41s (1500 15%) 2.4727]\n",
      "Whis, comyoull thoucourarin t waswhe I t serpove illa, me, t s y I h chave.\n",
      "RI l ll wr s, pesthe wo d  \n",
      "\n",
      "[0m 43s (1600 16%) 2.4344]\n",
      "Whado cou.\n",
      "Catoulen, nd, INCore\n",
      "Fest hindomorend mo I goung ve t me ag tin; har hyous;\n",
      "PThere frd theo \n",
      "\n",
      "[0m 46s (1700 17%) 2.4272]\n",
      "Wh f fumor.\n",
      "I ork inghyothe matherime heallder be s and he gill ald be, be an, tastilverd and wieto t  \n",
      "\n",
      "[0m 48s (1800 18%) 2.4445]\n",
      "Whes murus,\n",
      "Wat msiteristhe menghid I winere way h p' hof m be l, mowit l beteldodin.\n",
      "When jourd havio \n",
      "\n",
      "[0m 51s (1900 19%) 2.3959]\n",
      "Wh he'chase than at usk,\n",
      "O:\n",
      "M:\n",
      "Theses t atiles ceangr tuse me d thathalisus talare s was.\n",
      "Bersaly ou J \n",
      "\n",
      "[0m 54s (2000 20%) 2.4834]\n",
      "Whefe; t pen move RD t s; time as t.\n",
      "Thacethin,\n",
      "\n",
      "Whed me ithe t s t pe win k,\n",
      "ARo wis oucoucino t owit \n",
      "\n",
      "[0m 56s (2100 21%) 2.4523]\n",
      "Whouf thour yolior molinerou temy ng asheat g m ou se voford tor's oureand I ursthard ff br s s tour m \n",
      "\n",
      "[0m 59s (2200 22%) 2.4781]\n",
      "Whase An twin; myoulls oung Pethe therdyor: t weithe therk te h fe myoo foninorea seles n I ne wit sha \n",
      "\n",
      "[1m 2s (2300 23%) 2.5050]\n",
      "Whis,\n",
      "He t thindiurempre ther the:\n",
      "Asallepofou angs, it thif there s co t w w thatont, belldo to, veag \n",
      "\n",
      "[1m 4s (2400 24%) 2.4974]\n",
      "Whore mithtirs ayo tofoul y I le ane case heinitoma tll' ICINOLar a Bed us\n",
      "ARBIThithy.\n",
      "G comasseshatha \n",
      "\n",
      "[1m 7s (2500 25%) 2.4476]\n",
      "Whesthiman oma oung inchishiss fosthe ngofo y au he s marede cciches u d ch, at t s it anaret wars? in \n",
      "\n",
      "[1m 10s (2600 26%) 2.4515]\n",
      "Whe or he aille m s siee f gean the ake y st an the?\n",
      "Heng y:\n",
      "Thended and at hirthisin f pale t he y no \n",
      "\n",
      "[1m 12s (2700 27%) 2.4262]\n",
      "Whald e s sll ghothe but, thee fabal t l se ck lo ck s menderonge iecthomal the f t bin ING want fouro \n",
      "\n",
      "[1m 15s (2800 28%) 2.4908]\n",
      "Whees t\n",
      "CAny barintis imemy omprmy theree iormak tler ne u ur th stind\n",
      "RUMERUDULe thanneritherar th VO \n",
      "\n",
      "[1m 17s (2900 28%) 2.4642]\n",
      "Wh ay gs masatandetowit alleigr an thin whand chind r yot d, ye:\n",
      "KE:\n",
      "Hed\n",
      "tardin myoriselloulerd ichon  \n",
      "\n",
      "[1m 20s (3000 30%) 2.4562]\n",
      "Whand.\n",
      "maun tharorg a thes, ar rshath.\n",
      "PRI parscomy mounou d\n",
      "LARO, utha al wipre ousst S:\n",
      "Pas merke d  \n",
      "\n",
      "[1m 23s (3100 31%) 2.4756]\n",
      "Whoougrd ENThy, pentharallling heay we ckes win de he. s.\n",
      "Whean wisuthe th iche brodwithis, t,-han'ste \n",
      "\n",
      "[1m 25s (3200 32%) 2.5075]\n",
      "Why wn mif as.\n",
      "Touesehe ud y hele d ir oure w wo I'se as the fitarork?\n",
      "\n",
      "Buit t tholl is s?\n",
      "ENGofte be  \n",
      "\n",
      "[1m 28s (3300 33%) 2.4272]\n",
      "Whe d g\n",
      "\n",
      "Aner whole he u; ghitheril p hy theringors.\n",
      "ENG fr t:\n",
      "TI th won nord are ar matyeathave!\n",
      "NThe \n",
      "\n",
      "[1m 31s (3400 34%) 2.4460]\n",
      "Whathers, bou wendethe s, lly t my ICo momy overemit s an ll a theearomal 's alathe, I yougutly lenous \n",
      "\n",
      "[1m 33s (3500 35%) 2.4884]\n",
      "Whawoursthed batountt arees m fut n me merdishe urst ge suthin t aree the s llout\n",
      "DULAndinghr plor cak \n",
      "\n",
      "[1m 36s (3600 36%) 2.4423]\n",
      "Whe!\n",
      "Whourestour w menowarougs y sthesititove the,\n",
      "KI t yor wourre see his oratimare y wend s there on \n",
      "\n",
      "[1m 39s (3700 37%) 2.5027]\n",
      "Whay My kn d ad aieserepano isthese s n hess the\n",
      "I m.\n",
      "Ifoulinthathand s:\n",
      "DUE:\n",
      "NThis havere my ne s\n",
      "GAn \n",
      "\n",
      "[1m 41s (3800 38%) 2.4287]\n",
      "Whofer blericke yo t-ld, whabr an iso mowse. panouphes bene t s s co beritho tond, winoner,\n",
      "\n",
      "LATy anou \n",
      "\n",
      "[1m 44s (3900 39%) 2.5243]\n",
      "Whe ed anorithe we s t Gomefodaren wim wit nt d ourer' henthe by t alind tom hthe?\n",
      "Tis!\n",
      "Whansof st nd, \n",
      "\n",
      "[1m 47s (4000 40%) 2.5195]\n",
      "Whin, tisathere ou IShe he INIOf ndou m, ty tou phe in n ithorene, houraren I g owind l teatere atht l \n",
      "\n",
      "[1m 49s (4100 41%) 2.4539]\n",
      "Whearllir'sillof pe suthond of I n f ne ho ghatyougrvil hes t id by thes, y nd waio whoutrthoul d y t  \n",
      "\n",
      "[1m 52s (4200 42%) 2.4502]\n",
      "Wher th bl y r pe she, my wht weprors\n",
      "GAnd tth coue saily.\n",
      "\n",
      "And hed thishe wacour atithimyigonketheane \n",
      "\n",
      "[1m 55s (4300 43%) 2.4618]\n",
      "Whie th-\n",
      "\n",
      "An.\n",
      "Y t t wainoto the, sse, acrd ty ache aced the ale at ndis:\n",
      "'dldeakinos ckepr,\n",
      "Whand wo a \n",
      "\n",
      "[1m 57s (4400 44%) 2.4477]\n",
      "Whin at ita f y looy hatil owhame aliut. s sithard th hath pioure ar h g bl k; al ivigil f 'lllid s t  \n",
      "\n",
      "[2m 0s (4500 45%) 2.4468]\n",
      "Wharen t hengily jese becet go hind hestorer sthispis o sin\n",
      "Gekngho ndo f s fa st wid amunon ncovomaga \n",
      "\n",
      "[2m 3s (4600 46%) 2.4918]\n",
      "Whas, sple s wound s h the t, lon ht he ssousayongen.\n",
      "Anatthest ghe o s s st s ik, and oun, sacllle cc \n",
      "\n",
      "[2m 5s (4700 47%) 2.4690]\n",
      "Wha th meear t r? wor sher his\n",
      "\n",
      "WI MIOForomang tondared t ce, isowicofont'sowithoo win sth t.\n",
      "\n",
      "Forer;  \n",
      "\n",
      "[2m 8s (4800 48%) 2.4953]\n",
      "Wher, the.\n",
      "I s I wit ander t s s y s,\n",
      "Than t w, ir thothaur broree; as alande meat ilkist e aruremur n \n",
      "\n",
      "[2m 11s (4900 49%) 2.3834]\n",
      "Wh wan gat bl, belo t no y! aldis ope, I thar shetsant an we ureretand mest, the minatous.\n",
      "K:\n",
      "OMan:\n",
      "\n",
      "B \n",
      "\n",
      "[2m 13s (5000 50%) 2.4394]\n",
      "Whull plothill he\n",
      "\n",
      "RIO:\n",
      "Th m e awiat erore trile, is s be tstas o o thisilom nd theee LUCUConosind may \n",
      "\n",
      "[2m 16s (5100 51%) 2.4810]\n",
      "Whinds hare d\n",
      "I he ed l s: thathe sce wngovin.\n",
      "Se bes towhe a 'stow, ld'dd me aso wom t for hierr bend \n",
      "\n",
      "[2m 19s (5200 52%) 2.4634]\n",
      "Whe s, is lope, mellouch wise cer me:\n",
      "A:\n",
      "VIUS:\n",
      "DUK:\n",
      "'ds f he are waneldour tely t ymis.\n",
      "\n",
      "Win;\n",
      "IOr he a \n",
      "\n",
      "[2m 21s (5300 53%) 2.4370]\n",
      "Wheantcofout e h whacy bed ge\n",
      "MELLI f iforisthy'she ag the at bl acerefor memes gomad wom d my oworkis \n",
      "\n",
      "[2m 24s (5400 54%) 2.4988]\n",
      "Whamyow mimer f thir t wan\n",
      "Wh pr CAng, fawient,\n",
      "Bemas ment wat cengr ped thacong, s thar s waing.\n",
      "CHea \n",
      "\n",
      "[2m 27s (5500 55%) 2.4622]\n",
      "Whath n thour t!\n",
      "\n",
      "Burad s ntatathaver thend maner\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "KI t gel GHE:\n",
      "Fr g ll blery ighinyourdr bes thin \n",
      "\n",
      "[2m 29s (5600 56%) 2.4938]\n",
      "Whe d chefe s:\n",
      "AULANAnde Y ane s, de ss thars arelorand y ceny mer thont thousilllldeale'romisu,\n",
      "DUCEB \n",
      "\n",
      "[2m 32s (5700 56%) 2.4549]\n",
      "Who his, my the pprer e athatis t yoous y houbly, gorthe, l RUKERO:\n",
      "O:\n",
      "\n",
      "AUCARS:\n",
      "Brth,\n",
      "\n",
      "STheay hine the \n",
      "\n",
      "[2m 35s (5800 57%) 2.4404]\n",
      "Whe heered he t t oll t pro y ghit bee fort\n",
      "\n",
      "Toust t:\n",
      "I thevepe te thit s-\n",
      "INCHongit sthelle te on thi \n",
      "\n",
      "[2m 37s (5900 59%) 2.4781]\n",
      "Whth o, pes.\n",
      "I\n",
      "We thimathilot tes me Wheeafimy t\n",
      "\n",
      "Fithell belloreang thike waro s y-\n",
      "Or a hy I he the  \n",
      "\n",
      "[2m 40s (6000 60%) 2.5130]\n",
      "Whemy,\n",
      "Mandenownin me, th ther.\n",
      "\n",
      "Pr berthanef fr, isull benou s ale, t thenol, thowayoue ay amarilothe \n",
      "\n",
      "[2m 42s (6100 61%) 2.4755]\n",
      "Wheser atore th the whather and meilyon r win\n",
      "Yea icous binot be shistr powind mere.\n",
      "LABu the te ne hi \n",
      "\n",
      "[2m 45s (6200 62%) 2.4731]\n",
      "Whe oss s tondengre wosese w s ar st inoonediod hofare s t be asthe Mathe. wistepe masouthangr s gre r \n",
      "\n",
      "[2m 48s (6300 63%) 2.3925]\n",
      "Whove sthiel co t HERDUCo in mer ces acou s d tonor pthacto m ore s ar wimal s whitor at hinthincon t  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2m 51s (6400 64%) 2.5111]\n",
      "Whe usortacr bed, t wiminthowe; h he onort ter ar the wfe go ke o, cowe t, wase, t ge je the, fou st l \n",
      "\n",
      "[2m 53s (6500 65%) 2.4663]\n",
      "Whe mur and s, hathe chainggane twieristhawere it.\n",
      "\n",
      "Whe a the hieyo s he ghee tirenorverear th, fousou \n",
      "\n",
      "[2m 56s (6600 66%) 2.4231]\n",
      "Whou ay IS: ppen hal bu t d\n",
      "He he hes be lllot the I Ang wnd mere t sapll be are us bll d he ckeasthe  \n",
      "\n",
      "[2m 59s (6700 67%) 2.4848]\n",
      "Wheathinsomelonss the pthund heanolas pre ll co s lenig ar t n! hel yond:\n",
      "Ishofith CAn thosaprd be ry  \n",
      "\n",
      "[3m 2s (6800 68%) 2.4889]\n",
      "Whe eangr te wistilll the athens men ou s.\n",
      "\n",
      "If I orenitht and gr han ay, fou by t, mand y the yote wen \n",
      "\n",
      "[3m 4s (6900 69%) 2.5242]\n",
      "Whenghing toriru tifond y I forisen biticowiry e brt abrt has pe west, m inguris oul t witige, ce I an \n",
      "\n",
      "[3m 7s (7000 70%) 2.5193]\n",
      "Whe, ou awhee ders.\n",
      "I Busthere\n",
      "K:\n",
      "\n",
      "\n",
      "Nur'dre pe ped ckisare theldo the Cins t ighanghenshendse t lun st \n",
      "\n",
      "[3m 10s (7100 71%) 2.4794]\n",
      "Whan s str areris s, yo ponds metouthereastour whad tha wereeckelle thages:\n",
      "Thed wit IN thand rsold be \n",
      "\n",
      "[3m 12s (7200 72%) 2.4609]\n",
      "Whenditore iveeain iousor mu l ke on I tommyo t the s he se nd gr,\n",
      "CKI aig he aprisomemy our le d n\n",
      "\n",
      "s \n",
      "\n",
      "[3m 15s (7300 73%) 2.4856]\n",
      "Wht peth pl. shis obl thand sape.\n",
      "\n",
      "BRKELAnt bay's chien gowarpeit y\n",
      "NRD sth, t no eserousth, sisen the \n",
      "\n",
      "[3m 18s (7400 74%) 2.4639]\n",
      "Whaneit hed; moustolithes hes the nd t my withed whe hat mitharces be prain pl indomed ad trin theed v \n",
      "\n",
      "[3m 20s (7500 75%) 2.4793]\n",
      "Whase hidor he?\n",
      "A:\n",
      "To t e d thast nerd lan ines h s chas g ond mindint hane hye my\n",
      "OUERCou IUSo thenou \n",
      "\n",
      "[3m 23s (7600 76%) 2.4855]\n",
      "Whotindur e d crt.\n",
      "METha oralo byorthexpaybyowsth s sent wisertoumyo hel d hakig we, ofore wofa meat s \n",
      "\n",
      "[3m 26s (7700 77%) 2.4804]\n",
      "Whe s h racute sheng and wind leis m,\n",
      "IEENath or athind ighee hirl he pe stce y hicandseeat The m,\n",
      "ag  \n",
      "\n",
      "[3m 28s (7800 78%) 2.5623]\n",
      "Whis meat\n",
      "Find\n",
      "\n",
      "AREMa wh bre themes th t ande mo scommer thear cks ty s be t wo mber s le bll fo chath \n",
      "\n",
      "[3m 31s (7900 79%) 2.5161]\n",
      "Whapanoude\n",
      "Toll ay owetof mesa st ath somu wid hayoustheru y be t RO:\n",
      "the,\n",
      "WAnd t'd iffrou.\n",
      "Who tse t  \n",
      "\n",
      "[3m 34s (8000 80%) 2.4510]\n",
      "Whe wourousbr men, witit to butallly t aththathir miganghy y\n",
      "\n",
      "\n",
      "Mon wawano l y pare t londr, hendat pr, \n",
      "\n",
      "[3m 37s (8100 81%) 2.4782]\n",
      "Whand ithim t Ting rkean f this nt prin tin nowe,\n",
      "Herithin S: ppr tinnkellee four anthar t hindelithan \n",
      "\n",
      "[3m 39s (8200 82%) 2.4595]\n",
      "Where r ise d\n",
      "\n",
      "Toud heeeande I yone he ir ngod his me s ru The teathe fff seake lorillllong:\n",
      "Whend,\n",
      "Th \n",
      "\n",
      "[3m 42s (8300 83%) 2.4844]\n",
      "Whanag\n",
      "FLO s ther meithsidse t cis l melllord sor.\n",
      "s, mo y ains hor alls ave!\n",
      "Thare, m' ce tion.\n",
      "CUSPU \n",
      "\n",
      "[3m 45s (8400 84%) 2.4158]\n",
      "Whendithare havece hengrs aro as womaireshe, che my bee be t te pobulyou me loueath w heve,\n",
      "\n",
      "And, aned \n",
      "\n",
      "[3m 47s (8500 85%) 2.4493]\n",
      "Whanente s the tacct be be all hict me here t mist tainsise loon nthe.\n",
      "ARO:\n",
      "II y!\n",
      "\n",
      "\n",
      "\n",
      "ARDioundou y n ou \n",
      "\n",
      "[3m 50s (8600 86%) 2.4658]\n",
      "Whasad jou ve'llat\n",
      "\n",
      "CAngheaceet su hice glmpur bealeldanes se the eklean Mice s.\n",
      "Thathe\n",
      "Andoulds y t a \n",
      "\n",
      "[3m 53s (8700 87%) 2.4692]\n",
      "Whame mesalaran, s ndengs t wn s\n",
      "A:\n",
      "' ithavo, ougrenongu ledous Ifferint.\n",
      "They llkn thelldd ar wendath \n",
      "\n",
      "[3m 56s (8800 88%) 2.4411]\n",
      "Whimoomerrith I' the hothule f w tity the tof fas abe f t isoour theno l'sithand t mothaly as afrrak y \n",
      "\n",
      "[3m 58s (8900 89%) 2.4926]\n",
      "Wh s; henof I I s? d my, dise thos rontishoucr,\n",
      "As heppin he,\n",
      "S:\n",
      "ANGLICORD:\n",
      "And motond we d jous brcan \n",
      "\n",
      "[4m 1s (9000 90%) 2.4900]\n",
      "Wheead; othatlee angen: t st ague f y kensthes furat and an y, s LERLon s ckerdel s the alist'thenses, \n",
      "\n",
      "[4m 4s (9100 91%) 2.5528]\n",
      "Whe pe or in thiatamant d ather har perf utces inthitroresorpisth or t LAPE tiamuly w lds t blor s ee  \n",
      "\n",
      "[4m 6s (9200 92%) 2.4298]\n",
      "Whan l bimelde hy s d brerce ad his de nore n.\n",
      "\n",
      "Whabe ws ut mp iend by t me con f wer thind, s, les, a \n",
      "\n",
      "[4m 9s (9300 93%) 2.4290]\n",
      "Wheif ser.\n",
      "Bupeaiom cito he tithines thil thalodishers ce orary I\n",
      "CHINo t:\n",
      "\n",
      "Pe w'soresusta thar teso a \n",
      "\n",
      "[4m 12s (9400 94%) 2.4255]\n",
      "Wherwind halonelll he\n",
      "CE bllker E r, an se Rist thane cothyo t fer fl thathencowep illatheroutinerdan  \n",
      "\n",
      "[4m 14s (9500 95%) 2.4632]\n",
      "Wharere cerll EYouncheeache.\n",
      "CHelisee she sthent is! tore athan mers je ppure pourttheth inon wau, pou \n",
      "\n",
      "[4m 17s (9600 96%) 2.4779]\n",
      "Wheaveshe we, n s'llsthave for'lol s bore be myoursicausther fit, thanghtereree der te gar me the inth \n",
      "\n",
      "[4m 20s (9700 97%) 2.4529]\n",
      "Wh ce nd meleemas ak the ma ple s foyomyof s s oraie thaver ofaverdey;\n",
      "INCK:\n",
      "NGouler t y at n he sove  \n",
      "\n",
      "[4m 22s (9800 98%) 2.4552]\n",
      "Whe me arsthend\n",
      "\n",
      "I arof hy y theallafan, aindred me our, eateached s ckeconch sifu hed thef s anod.\n",
      "Th \n",
      "\n",
      "[4m 25s (9900 99%) 2.4615]\n",
      "Why I f boul f bllfancoran s thiny mer pr mer ou I's hell home; hespe s himy mithe, IUThin ing ime ary \n",
      "\n",
      "[4m 28s (10000 100%) 2.4858]\n",
      "Whea theacherthas:\n",
      "Couppees str wnt m harwed wir O:\n",
      "BRYO ie tm hathallm, he,\n",
      "ARERMEThy weste be thart  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "###Parameters\n",
    "n_epochs = 10000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 5\n",
    "lr = 0.005\n",
    "batch_size = 16\n",
    "chunk_len = 80\n",
    "\n",
    "####\n",
    "\n",
    "model = RNN(n_characters, hidden_size, n_characters, n_layers) #create model\n",
    "model_optimizer = torch.optim.Adam(model.parameters(), lr=lr) #create Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss() #chose criterion\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "def train(inp, target):\n",
    "    \"\"\"\n",
    "    Train sequence for one chunk:\n",
    "    \"\"\"\n",
    "    #reset gradients\n",
    "    model_optimizer.zero_grad() \n",
    "    \n",
    "    # predict output\n",
    "    output = model(inp)\n",
    "    \n",
    "    #compute loss\n",
    "    loss =  criterion(output.view(batch_size*chunk_len,-1), target.view(-1)) \n",
    "\n",
    "    #compute gradients and backpropagate\n",
    "    loss.backward() \n",
    "    model_optimizer.step() \n",
    "\n",
    "    return loss.data.item() \n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len,batch_size))  #train on one chunk \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(generate(model,'Wh', 100), '\\n')\n",
    "       \n",
    "\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 3s (100 1%) 2.5406]\n",
      "Whid 'soncor t tod sOROLAUCEULO pend otho hou honor ind peante wisecon,\n",
      "D tr fat bourne borit t t is t \n",
      "\n",
      "[0m 7s (200 2%) 2.4725]\n",
      "Wharist that, mof fos he hebeearin ol semesedevistheato myoone, Iwout t chens he derofreemy.\n",
      "TI mereav \n",
      "\n",
      "[0m 11s (300 3%) 2.5172]\n",
      "Whe st, the that ithe serin, t s t 's avens,\n",
      "I d\n",
      "The the t tha thy fou wilal, theno lis:\n",
      "I cafo l fod  \n",
      "\n",
      "[0m 15s (400 4%) 2.5572]\n",
      "What methe fo y, ast l linathy e as;\n",
      "\n",
      "INAn minouthe scapl aroresp mort,\n",
      "\n",
      "S:\n",
      "I'd teelo wisenthe t men I \n",
      "\n",
      "[0m 19s (500 5%) 2.4529]\n",
      "Wheay :\n",
      "To y t y ck moas\n",
      "MORUCHe\n",
      "CUENV:\n",
      "GBe wourge e bour here the, icherok m chage t thitetr bes y th \n",
      "\n",
      "[0m 23s (600 6%) 2.4754]\n",
      "Whof ntat ee wous RELeses CUSToriry s w ir may my I t oo r w t coury farye wit Yof CARGit of s thes wi \n",
      "\n",
      "[0m 27s (700 7%) 2.5149]\n",
      "Whe at aushathowacofou, t ha th d de he t s at t be.\n",
      "A mag e h t st as fu s thed?\n",
      "Wngur w, murase I s; \n",
      "\n",
      "[0m 31s (800 8%) 2.5136]\n",
      "Whershenes and cale uculo mhanyot ilsthord, orend was te, l nshe, ondeerug gbld, med nealyousand thand \n",
      "\n",
      "[0m 35s (900 9%) 2.5392]\n",
      "Whero inor LUS:\n",
      "And; s he she ond be:\n",
      "\n",
      "\n",
      "Por mie od oru d, muthay ithakelorouround t aimyorothan ber,\n",
      "A \n",
      "\n",
      "[0m 39s (1000 10%) 2.4242]\n",
      "Whithatre desthirth hy are swan at thatire athtindoullotare d d s t'l g th tou pasopove ave y fe n!\n",
      "T: \n",
      "\n",
      "[0m 43s (1100 11%) 2.3902]\n",
      "Whe holal t yoofay s nd w so momavemye,\n",
      "MENVI'thallo ble fomomon d me be n'de,\n",
      "Whe f ay m falyorer ame \n",
      "\n",
      "[0m 47s (1200 12%) 2.4662]\n",
      "Wh thend hiepererss h ar,\n",
      "Ang morditaliked wingdin yo alllcosthe hechispor Anshe htheame s THend wayor \n",
      "\n",
      "[0m 51s (1300 13%) 2.5271]\n",
      "Whe n par,\n",
      "The ice,\n",
      "And harke merdokit w be m id.\n",
      "SI pe; pl t Ind; fount athe ptelat f d blive why is, \n",
      "\n",
      "[0m 55s (1400 14%) 2.5203]\n",
      "Whemy ed, t lead wery lethe 'ly, dof ald s athaly ase fowimeaw n wale ba ndy ealto tomonthy; ce fered  \n",
      "\n",
      "[0m 59s (1500 15%) 2.5039]\n",
      "Whe chind. dd!\n",
      "BAR ghante,\n",
      "Tan fod w thanerrin f I thederindonge I isecoyoond.\n",
      "\n",
      "KICHive m y ferthethei \n",
      "\n",
      "[1m 3s (1600 16%) 2.4793]\n",
      "Whacke win to there hes shit he thamy than u bu II bubein tourte wmamoshemine, sh mee hem ther s.\n",
      "ADUC \n",
      "\n",
      "[1m 7s (1700 17%) 2.4697]\n",
      "Wheadaist t and toren my r,\n",
      "\n",
      "A ie be?\n",
      "Dind haunt, s glinous the.\n",
      "CATHAREY tthel mul s thyor m tromouro \n",
      "\n",
      "[1m 11s (1800 18%) 2.4566]\n",
      "White thr th hau,\n",
      "\n",
      "IIfus a han aller prsor y her he wo ay ld me dind oref w badis t s ltyoualil h t th \n",
      "\n",
      "[1m 15s (1900 19%) 2.4401]\n",
      "Whif woucint'le CHAng theathe s d lo ld mishigllf ashifre te f th omyouie w fo e hes y hindit faneplea \n",
      "\n",
      "[1m 19s (2000 20%) 2.4625]\n",
      "Wh d a welind l ness borer ts\n",
      "Pid me tolyo be.\n",
      "YO: perero f, ve, ss andouf ofoour, wildoug impl we thi \n",
      "\n",
      "[1m 23s (2100 21%) 2.4793]\n",
      "Wheeanel y his, I t banguspe ike, ay thareinchancke meacay thoreany sag tlll Gl, in'Thest arend mere m \n",
      "\n",
      "[1m 26s (2200 22%) 2.4763]\n",
      "Wh weanoulioupe'd I a\n",
      "TEOxt myod be this ar w\n",
      "S:\n",
      "Thive ngoffan ker irem nomathette paramen ind! gle ce \n",
      "\n",
      "[1m 30s (2300 23%) 2.4118]\n",
      "Whe aig arallels ane in V: t angot h hi! k he tullald co hatca de the coowil ING aty mere aso bl anor  \n",
      "\n",
      "[1m 34s (2400 24%) 2.4047]\n",
      "Whist icor rd anond Calond n moueinersire,\n",
      "O,\n",
      "\n",
      "NCEI:\n",
      "Whe h pr yorith nd myofr is te derren hetr d mati \n",
      "\n",
      "[1m 38s (2500 25%) 2.4523]\n",
      "Whianore y ang ndouthis linouthesoow l woupeth s alldwe hinthigle\n",
      "\n",
      "\n",
      "Wh alure t mas is, p owe w tovinor \n",
      "\n",
      "[1m 42s (2600 26%) 2.4274]\n",
      "Whe cof m tor, be in. thers wos indu\n",
      "Tintaid theal Bithofouresthack'pthathon 's.\n",
      "\n",
      "D f, ano re se me.\n",
      "H \n",
      "\n",
      "[1m 45s (2700 27%) 2.4424]\n",
      "Whand,\n",
      "Bu ve arooul anthole thar's:\n",
      "\n",
      "\n",
      "\n",
      "BENGEThaken ond ace\n",
      "VO, sthin t t d thesumanclid I bre as ate m \n",
      "\n",
      "[1m 49s (2800 28%) 2.5018]\n",
      "Whice we l y lfak a\n",
      "Angofot an e girst tof nomer?\n",
      "SCAn rsou hendo borse gone bllior tr com she s omy y \n",
      "\n",
      "[1m 53s (2900 28%) 2.4824]\n",
      "Wh t contis iss th thent tor touce toro t t,\n",
      "ins me ld:\n",
      "Gonend my malengtho ichamelishenonthe me l tan \n",
      "\n",
      "[1m 57s (3000 30%) 2.4727]\n",
      "Wha Head hind br ico wothan, t,\n",
      "GRThewhif browime I d d a Whanourea INUDe f fon;\n",
      "\n",
      "Erece\n",
      "To w bomy! t f \n",
      "\n",
      "[2m 1s (3100 31%) 2.4517]\n",
      "When f Pr woues d t min orara azen ticue's'sour ase d and he t le t!\n",
      "QUCosturerone ssthad t le Herour  \n",
      "\n",
      "[2m 5s (3200 32%) 2.3918]\n",
      "Wharingthaly y hicou bl s han arce.\n",
      "G cowhesour the, Sethy meat ineth and y, paninde benghenchice I'sh \n",
      "\n",
      "[2m 9s (3300 33%) 2.4532]\n",
      "Who t macond prindvestharithathis towinere ay athour al e thovesstind w'Thithoutt war het w;\n",
      "TENENI be \n",
      "\n",
      "[2m 13s (3400 34%) 2.4850]\n",
      "Whe m ay d be f t wath a had ke seds nd, athard t Theat I bimest.\n",
      "AULOLI mey owingucheit t tes gakilli \n",
      "\n",
      "[2m 18s (3500 35%) 2.4758]\n",
      "Whell ngins bs m, hes br nno whed g mbeise I o'se s ttwal my ong ponowh s g w towing hindiay is f myo  \n",
      "\n",
      "[2m 22s (3600 36%) 2.4643]\n",
      "Whe an busouscl outhe\n",
      "Anealloseanon anee bes ha s ll, an ssou songor, my hathocherawouthest ben ppe qu \n",
      "\n",
      "[2m 26s (3700 37%) 2.4506]\n",
      "What hararfade h ast?\n",
      "\n",
      "\n",
      "HAn in chanen t.\n",
      "And s,\n",
      "NAN!\n",
      "Pr is man, ilisiese ghanot o t?\n",
      "\n",
      "\n",
      "A co a hecht,\n",
      "A \n",
      "\n",
      "[2m 30s (3800 38%) 2.4906]\n",
      "Wharthe toway, wher he inge t mis abt sp:\n",
      "Windstil horinch but ngrsefouthethe n buthet, cat d iteethat \n",
      "\n",
      "[2m 34s (3900 39%) 2.5194]\n",
      "Whoule quou ore we m bllll f imeve hin aseathe my ge y m by bel st busle f S:\n",
      "A omu whoumest h cthart  \n",
      "\n",
      "[2m 38s (4000 40%) 2.4003]\n",
      "Why this te hand.\n",
      "CAn weat nginor?\n",
      "PELPUKETh wo fe, keene omingiselld we thonth wascebocan ar wicha ha \n",
      "\n",
      "[2m 42s (4100 41%) 2.4491]\n",
      "Whad t yorpr hurstwistreayo s seryoun, pllller f sa thin, hasthasthewngherws as t the! d ho ang horeth \n",
      "\n",
      "[2m 46s (4200 42%) 2.4925]\n",
      "Whes cos n cove inere denghis f RUKEThut tofean ale herpe cthone anothor.\n",
      "\n",
      "\n",
      "Fieemyorve herthon pee t c \n",
      "\n",
      "[2m 50s (4300 43%) 2.4473]\n",
      "Whiting.\n",
      "LAn.\n",
      "\n",
      "I ro Got's burif, bondersk iand his ly t:\n",
      "Ancke I ste meean are l dlled ourllof my lous \n",
      "\n",
      "[2m 55s (4400 44%) 2.5065]\n",
      "Whe ok tetil wie s y ary s t or l llingat spaices iterd,\n",
      "DUS:\n",
      "Thy twowit bock sesorus therowayo wathy  \n",
      "\n",
      "[2m 59s (4500 45%) 2.4762]\n",
      "Wher nt chothea se!\n",
      "ild t mo ne ame ngrd w roud f t y,\n",
      "\n",
      "Wimeatous waronn.\n",
      "\n",
      "RO: blir,\n",
      "De s cam the.\n",
      "\n",
      "hi \n",
      "\n",
      "[3m 3s (4600 46%) 2.4703]\n",
      "Wh t dit.\n",
      "I theeisthin wheret\n",
      "Thist t w ateate itre sore ce fthorkn mes.\n",
      "Yor atholer fouleringe mis ga \n",
      "\n",
      "[3m 7s (4700 47%) 2.5122]\n",
      "Whe t'ld ne r,\n",
      "Be theeacetour furous he:\n",
      "win t\n",
      "As gie yout wil g'the thellll seand be ndendse we fes\n",
      "T \n",
      "\n",
      "[3m 11s (4800 48%) 2.4600]\n",
      "Whe ore KIOff whisth lond telllthurs trd ganthefr:\n",
      "SALO:\n",
      "\n",
      "S:\n",
      "Gofinindowan gers rerd plll mmepldin s nd \n",
      "\n",
      "[3m 15s (4900 49%) 2.4887]\n",
      "Whie by-ce is cinaile oulan he the r t toppllas rean wo angearres t kining us ord p wh f m avithrer, o \n",
      "\n",
      "[3m 18s (5000 50%) 2.4378]\n",
      "Whapate, de me wipe t il mourome chullle te,\n",
      "ANGLI l co alouce whenge ary my s te the-\n",
      "O:\n",
      "D terinen m, \n",
      "\n",
      "[3m 22s (5100 51%) 2.5215]\n",
      "Whuro,\n",
      "Thato tit f cen'diny g an ber'dorirein\n",
      "O:\n",
      "\n",
      "A:\n",
      "\n",
      "TII the, fisis;\n",
      "\n",
      "PENorer he I moke akild he chen \n",
      "\n",
      "[3m 26s (5200 52%) 2.5589]\n",
      "Wh y fan.\n",
      "\n",
      "Pe, f se s towhan tr IO:\n",
      "And moube'lll nk y has mont is s thisem s hang tho he ld eathat he \n",
      "\n",
      "[3m 30s (5300 53%) 2.4920]\n",
      "Whyot blthe the, st cty s he l I, id ou lathine ISAned\n",
      "Theish ivetedit old bin se the u, tir, y, ped h \n",
      "\n",
      "[3m 34s (5400 54%) 2.5337]\n",
      "Whate and me oy bulor s sat yet cor aneens wingrd we SS:\n",
      "Thend ner d njure he ee blat lend t bansierar \n",
      "\n",
      "[3m 38s (5500 55%) 2.5248]\n",
      "Whendovat tes.\n",
      "KE:\n",
      "\n",
      "Woute ift inof lte acowndereles t heave whein, HI stichis at t:\n",
      "S: t then tothewil \n",
      "\n",
      "[3m 42s (5600 56%) 2.5412]\n",
      "Whe th f cand ys owe gell th byof I buss atay,\n",
      "ANCo-cheen me'thithams wexave m buthed in thiratoulak y \n",
      "\n",
      "[3m 46s (5700 56%) 2.5104]\n",
      "Wher f I fath, whe.\n",
      "\n",
      "Thand, ded--\n",
      "\n",
      "Thourdendend d aver ro thanollelle y ig t ta, hitie candig me hes,  \n",
      "\n",
      "[3m 50s (5800 57%) 2.5419]\n",
      "Whoremanthed t whimu l thameches int oan Got sines s mee mon:\n",
      "Tous ORYo schin m te bllye alie werirser \n",
      "\n",
      "[3m 54s (5900 59%) 2.3892]\n",
      "Whesofisus myou, owe d ccr s odin'sofad thingo An, akng\n",
      "Welofthe'thengouse, with averke ghounch;\n",
      "YOLLA \n",
      "\n",
      "[3m 59s (6000 60%) 2.4942]\n",
      "Wheat widonend hoorithast hinour s.\n",
      "VI t athe and fars e beau las ho's hare fere he be t ba l wer tre  \n",
      "\n",
      "[4m 2s (6100 61%) 2.4958]\n",
      "Wherither tin ce w lerd e, thu in t gur owerinon angl wend g he,\n",
      "AThorn monore m, thint s giexe hr in  \n",
      "\n",
      "[4m 6s (6200 62%) 2.4271]\n",
      "Wher imithen hend f msithed at s,\n",
      "Folete roncorarnknder ameve sth we! kicend matole d nirer noneerithi \n",
      "\n",
      "[4m 10s (6300 63%) 2.4568]\n",
      "Whar pllo d tel we t ber waver te ce t sikeverucag alle-ted bomy y s hande tis:\n",
      "ENGlve the ors gh anse \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4m 14s (6400 64%) 2.4867]\n",
      "Wherth tonimawo s orrend ast\n",
      "LAn co bus my tancherrtharot me whe t dond walit rimon tr d m, mand t bo  \n",
      "\n",
      "[4m 18s (6500 65%) 2.4422]\n",
      "Whirthed IO, ngare h st y t r t his kigitort bemiroswisear tind s thet mean mine ainou mimppprepr asce \n",
      "\n",
      "[4m 22s (6600 66%) 2.4763]\n",
      "Whe t arer, se the nt po hou, t, ty.\n",
      "HEELAne t, E:\n",
      "Find pe trs t inolde tinowhedis tort in htanou h an \n",
      "\n",
      "[4m 26s (6700 67%) 2.3734]\n",
      "Whimen ald y n waves aind ofert RAnd meseese titheary lindo s,\n",
      "He me hthe sthe he ad ttanchid f ICO:\n",
      "\n",
      " \n",
      "\n",
      "[4m 30s (6800 68%) 2.4951]\n",
      "Whaknes.\n",
      "\n",
      "PULAUSit wind I home trg s s! halll ime t s sererothatese min frsthe h o w ICAyo JUCit nteat \n",
      "\n",
      "[4m 34s (6900 69%) 2.4142]\n",
      "Whio s e m omy Is he M:\n",
      "Myowillllofou ouragerime bet somod are.\n",
      "O,\n",
      "O:\n",
      "\n",
      "antis ind se y whaide thtredeac \n",
      "\n",
      "[4m 38s (7000 70%) 2.4126]\n",
      "Whotomyofate fisthiorat d w y o fe mecad ot was s, l ng. y,\n",
      "Satood th my athe, ayot ave t Hata heno be \n",
      "\n",
      "[4m 43s (7100 71%) 2.4070]\n",
      "Whire tik cavere ch gre me totirare d d ie shes:\n",
      "\n",
      "I HE 'senothand,\n",
      "Fak the t port meathe gshee f y me  \n",
      "\n",
      "[4m 47s (7200 72%) 2.5034]\n",
      "Wh in mer be, yofoured s he rmin be spou thinsindearels, thand; ar andyonowhit sere lesar t ave DWere  \n",
      "\n",
      "[4m 51s (7300 73%) 2.4428]\n",
      "Whes; way cl bas s mayou torsorior chisser II pangelind f e. pre par t bage IADour mot anthathe,\n",
      "TI:\n",
      "W \n",
      "\n",
      "[4m 55s (7400 74%) 2.4808]\n",
      "Whasthare f wour:\n",
      "BENGHat,\n",
      "\n",
      "\n",
      "ENan n hee cenghel CE:\n",
      "Thang ar moubur wneak ithemowenord wh wanomeiman h \n",
      "\n",
      "[4m 59s (7500 75%) 2.4416]\n",
      "Whan mer we ticthe trdy theain.\n",
      "Fo s,\n",
      "Suren atoudie be n oure whor thon ve.\n",
      "\n",
      "Mar he te nd wiowhel ioua \n",
      "\n",
      "[5m 3s (7600 76%) 2.4140]\n",
      "Whate ce,\n",
      "\n",
      "IOUShend d pend thee d thigheftha\n",
      "APRS: was, d wor be! d,\n",
      "WIn\n",
      "G I gare e se s ber th s henc \n",
      "\n",
      "[5m 7s (7700 77%) 2.4605]\n",
      "Why w?\n",
      "Tomathe mee ay we f wat's t we ches I wo t d, twind y tor, t or at thendengst thandinge t th wi \n",
      "\n",
      "[5m 10s (7800 78%) 2.5005]\n",
      "Whevet coril oucoutl anerthesp the t th pamaincatred s;\n",
      "CESAMal HI's:\n",
      "S:\n",
      "\n",
      "\n",
      "Whe by t my f hr t thin wed \n",
      "\n",
      "[5m 14s (7900 79%) 2.4397]\n",
      "Whar corgandand of y'sheeg be notars fous,\n",
      "Bury bllld iny tht, wat fe mbeastiewenourd as anqurelldind  \n",
      "\n",
      "[5m 18s (8000 80%) 2.4975]\n",
      "Whins thor s,\n",
      "ENERESTharowhakee anghis y,\n",
      "\n",
      "Ayor t:\n",
      "Whiore het tar thathie t and thand tth akey ter pro \n",
      "\n",
      "[5m 22s (8100 81%) 2.4336]\n",
      "Whar pat se, et Anthed s gutoutaloof ty, thengond thir t thig, ninore whinm thiverong. mal, meno f sh, \n",
      "\n",
      "[5m 26s (8200 82%) 2.4737]\n",
      "Whethiss t,\n",
      "Thathe phyeve ser.\n",
      "Tourercous.\n",
      "I'sanoud bese tholld bas wou asouburemeresthive brot, avees \n",
      "\n",
      "[5m 30s (8300 83%) 2.5021]\n",
      "Wh th id manatheler thy s dackequthe I tld tas courillare mand toue mobl ayofs dorr, my gellld t mo t  \n",
      "\n",
      "[5m 34s (8400 84%) 2.4687]\n",
      "Wheowishis bld tofowon blerde.\n",
      "MBes IUTous?\n",
      "\n",
      "To hes thed avay paris hth shofe y 'd, bedllonghouthit w, \n",
      "\n",
      "[5m 38s (8500 85%) 2.4620]\n",
      "Whar we llece way h se in he'd ise alimyomow? migeathisth t atund her thea wheary, s t I thes al.\n",
      "GRET \n",
      "\n",
      "[5m 42s (8600 86%) 2.5190]\n",
      "Where INounol Mur thestothst I whonowe mis p I w atitrt' t menor isoube anep he che ansth toneasis,\n",
      "Tw \n",
      "\n",
      "[5m 46s (8700 87%) 2.4457]\n",
      "Wh s whes faverdol wis d sot, tre t s hacous ansiste m whe tieas ll wen. w wike ce we matoounou my,\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "[5m 50s (8800 88%) 2.4906]\n",
      "Whan fende are anericoude Irit re ckerare a ardewatofobed s thelowas; lout f maithin are, hies ity pe  \n",
      "\n",
      "[5m 54s (8900 89%) 2.4711]\n",
      "Whe ars k bedeshompherare hitughyo wisel ire br, t med,\n",
      "\n",
      "S:\n",
      "AMNCIENGSourathener\n",
      "Swotama.\n",
      "I ukesef be l \n",
      "\n",
      "[5m 58s (9000 90%) 2.5314]\n",
      "Whe\n",
      "Tor wink rour s llde nt,\n",
      "UCo nd ouleand h NG hexeact:\n",
      "\n",
      "Hets towieres, isco l.\n",
      "Tidrty t herd pay ur \n",
      "\n",
      "[6m 2s (9100 91%) 2.4936]\n",
      "Wheis thot cedan o y d.\n",
      "\n",
      "\n",
      "Anope whiend akno araru mourd tur yourly shorousen the tee dierend y s, k, a \n",
      "\n",
      "[6m 6s (9200 92%) 2.5067]\n",
      "Wht hate\n",
      "K:\n",
      "We s\n",
      "Beer: t hat nathe t he ato tand s wie ad bete cherolend mfrme\n",
      "FRoues ueis at sthelour \n",
      "\n",
      "[6m 10s (9300 93%) 2.3795]\n",
      "What grerim thand bbened atou, heybore?\n",
      "I hf yor are shenemede\n",
      "Ano f n mff prethin how t n g ilumangre \n",
      "\n",
      "[6m 14s (9400 94%) 2.5033]\n",
      "Whetht a tthigat\n",
      "\n",
      "PARDUShyo t s fe wovemave hot, ICis wese ly\n",
      "BENGEE:\n",
      "I alime wef ene de br,\n",
      "MA a tlpr \n",
      "\n",
      "[6m 18s (9500 95%) 2.4183]\n",
      "Whare; mese mmange blot m l'sonse, denean; ting sse bine tha thanoun t oousorisancathederse, llowasind \n",
      "\n",
      "[6m 22s (9600 96%) 2.5011]\n",
      "Whe t,\n",
      "\n",
      "CEve w llathorronoong woorof anou cerere tr yototheve houred the atiath uthand ond be tont hey \n",
      "\n",
      "[6m 26s (9700 97%) 2.5352]\n",
      "Whe.\n",
      "Thisthithat s;\n",
      "NGARCoupe; t t pre, hig ththir impe plar pathang mo he\n",
      "\n",
      "TERThupl me g r seve s s t \n",
      "\n",
      "[6m 30s (9800 98%) 2.5079]\n",
      "Whad anerind, sthiserelonourco boupene thisatifo fr to tant alow, wesowitandepr, sthtthorethiged ch to \n",
      "\n",
      "[6m 34s (9900 99%) 2.4238]\n",
      "Whinan t hesorick batin y bls an t od he mpeencar dat me fourthanouke men y habe m g p n amed t? benge \n",
      "\n",
      "[6m 38s (10000 100%) 2.4926]\n",
      "Whameay\n",
      "Whers d d peror n, w s he I's?\n",
      "MAnour re thathoullerd nr thto tht h tofine sur wincerinstanthe \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Parameters\n",
    "n_epochs = 10000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 5\n",
    "lr = 0.005\n",
    "batch_size = 16\n",
    "chunk_len = 80\n",
    "\n",
    "model2 = RNN(n_characters, hidden_size, n_characters, n_layers, nn.GRU) #create model\n",
    "model_optimizer2 = torch.optim.Adam(model2.parameters(), lr=lr) #create Adam optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #chose criterion\n",
    "\n",
    "start = time.time()\n",
    "all_losses2 = []\n",
    "loss_avg = 0\n",
    "\n",
    "\n",
    "def train(inp, target):\n",
    "    \"\"\"\n",
    "    Train sequence for one chunk:\n",
    "    \"\"\"\n",
    "    #reset gradients\n",
    "    model_optimizer2.zero_grad() \n",
    "    \n",
    "    # predict output\n",
    "    output = model2(inp)\n",
    "    \n",
    "    #compute loss\n",
    "    loss =  criterion(output.view(batch_size*chunk_len,-1), target.view(-1)) \n",
    "\n",
    "    #compute gradients and backpropagate\n",
    "    loss.backward() \n",
    "    model_optimizer2.step() \n",
    "\n",
    "    return loss.data.item() \n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len,batch_size))  #train on one chunk \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(generate(model2,'Wh', 100), '\\n')\n",
    "       \n",
    "\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses2.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 4s (100 1%) 2.5532]\n",
      "Whindr Y minl wine nourr her y fures he by ane mesoul w e Matrd ngre sevesen kelur sst f nd'u file wig \n",
      "\n",
      "[0m 8s (200 2%) 2.4603]\n",
      "Whisberericoreathou tha athelye d RD he s no o Be baispy here nlld,\n",
      "Wat fait ear doon at IAn d t hathe \n",
      "\n",
      "[0m 12s (300 3%) 2.4835]\n",
      "Wheleande me we be ous; womird y samy ly INGShitowir sis'the tarouithen fashad ngoue ve, be me g me ho \n",
      "\n",
      "[0m 16s (400 4%) 2.4806]\n",
      "Wh ther llerd youn t fakinofo coutat athy ave GAREge yt t I s ose taur thes irithe ch mowans; ot n, s  \n",
      "\n",
      "[0m 20s (500 5%) 2.4763]\n",
      "Whathelle my ans fore, witharulen y dirouthespefisous t aler tre, e pee llendun the hand ous su fo's E \n",
      "\n",
      "[0m 25s (600 6%) 2.4533]\n",
      "Whe wis r thys w wed, nkinger od y, stemy wamand, whenond thimer ther th:\n",
      "D abe thor fullce g ban win  \n",
      "\n",
      "[0m 29s (700 7%) 2.4743]\n",
      "Whating ce th thind ind ngndour toreth I ist whe har.\n",
      "Cals t she ient pu I ie hof orsl lowas.\n",
      "Anthe t  \n",
      "\n",
      "[0m 33s (800 8%) 2.5472]\n",
      "Whathererdisee at been osorote the bes emierit benges oin,\n",
      "Hin\n",
      "Pe be o w l, by heastouso,\n",
      "my tellar ou \n",
      "\n",
      "[0m 37s (900 9%) 2.4601]\n",
      "Whinjonyopeave chevin ar cowhave y hayore, sprneaicive han, hire moo pind banene.\n",
      "Hee t, bulowhre g wo \n",
      "\n",
      "[0m 41s (1000 10%) 2.3959]\n",
      "Whare, woworense othinaclpr a thoupun or-\n",
      "LISonsor y thenar II overiciche rolt thenden wn ate: ge mugo \n",
      "\n",
      "[0m 46s (1100 11%) 2.4750]\n",
      "Whin t s s der herou fir w. thit arr d he ie;\n",
      "Wh fowoul anore awakithitha we! wander be s.\n",
      "O:\n",
      "O ff ain \n",
      "\n",
      "[0m 50s (1200 12%) 2.4654]\n",
      "Wh LAndithoustin ye the he hised?\n",
      "CINRUED be frice! t' won in kends the\n",
      "BENoul,\n",
      "Tomyong papore, so he  \n",
      "\n",
      "[0m 54s (1300 13%) 2.4097]\n",
      "Wharthes t thanoue t ck t withad be lis,\n",
      "basiuninyong parghajonthans, r Bubene p t bord llithe r con t \n",
      "\n",
      "[0m 59s (1400 14%) 2.4317]\n",
      "Wheavothanoroutho arechthatin ay an,\n",
      "Mo thar icoune ber anmaueitthe thatoray t e se, I' INI r wingonar \n",
      "\n",
      "[1m 3s (1500 15%) 2.4568]\n",
      "Whe, tonds genout s fulvetome ind, frishe inour ou atofuther ceco the athenountore k.\n",
      "Buthis RCof,\n",
      "the \n",
      "\n",
      "[1m 7s (1600 16%) 2.4742]\n",
      "Whart of y the fetove thinereelou! me! wn t, cofow hy;\n",
      "CHousereal y sthatonow dsen d a mo iathe fout i \n",
      "\n",
      "[1m 12s (1700 17%) 2.5178]\n",
      "Whir s he se, thedover, ond pee ty, d orio anou thy d sous th pe s illis the bes tisouse tore yofo le  \n",
      "\n",
      "[1m 16s (1800 18%) 2.5282]\n",
      "Wh d am drad t o be tae?\n",
      "Singake sw isthed suthal's a d inde LA renour my d whedres.\n",
      "\n",
      "\n",
      "D we ato p, hue \n",
      "\n",
      "[1m 20s (1900 19%) 2.4556]\n",
      "Whes act;\n",
      "Whanont s But hico ce mere cours ha man datho.\n",
      "\n",
      "\n",
      "A:\n",
      "AN:\n",
      "Cis s whos bemaind hores.\n",
      "Anold s in \n",
      "\n",
      "[1m 24s (2000 20%) 2.4725]\n",
      "Whantharthere mudougend l sencu s thek m!\n",
      "\n",
      "Prine'\n",
      "\n",
      "\n",
      "An therenorithe acithero healllloos hry hees awhou \n",
      "\n",
      "[1m 29s (2100 21%) 2.4893]\n",
      "Whandon ayo w th bla imyous'dicat imeallld far are oseve ingh y he tot,\n",
      "LAnon ESond me Fo f is th y, w \n",
      "\n",
      "[1m 33s (2200 22%) 2.4903]\n",
      "Whe enikbit bie yo womybes\n",
      "Toucomesere some te th wis tinacopouarever nd\n",
      "QUCUSo Pra cke, araw\n",
      "Alofe r  \n",
      "\n",
      "[1m 37s (2300 23%) 2.4585]\n",
      "Whe\n",
      "\n",
      "OUKI d the ow ble othie'd sor ce mithalet bulelsh itld iny ss! hare as th pof t,\n",
      "INoricleris hed  \n",
      "\n",
      "[1m 41s (2400 24%) 2.4757]\n",
      "Whengearamf s the, t tes ithe hierealellle,\n",
      "\n",
      "SBRDULes:\n",
      "\n",
      "Whearisthe call hiomie?\n",
      "\n",
      "d l art t ffo-matitho \n",
      "\n",
      "[1m 46s (2500 25%) 2.4983]\n",
      "Whe bupofowhe thave, t foon ce os me, sthisur inghathe S: an hane hertheereare w s the whinos aind nth \n",
      "\n",
      "[1m 50s (2600 26%) 2.4814]\n",
      "Whe oun yo shesthecen'sthard qut wisty, thave hor Pe sal grot aman:\n",
      "Bh EThet in tan-n henthatird tofis \n",
      "\n",
      "[1m 54s (2700 27%) 2.4196]\n",
      "Wha out ous tharu t se INof the s atee thof\n",
      "TOne thise al gerd fou t atid, t f h theshans m 'ded marit \n",
      "\n",
      "[1m 59s (2800 28%) 2.5062]\n",
      "Whe gatr a ie t he.\n",
      "\n",
      "Whayous s t f arend auth 'dir alle tinghille he s ste y aneme he hispime h thotha \n",
      "\n",
      "[2m 3s (2900 28%) 2.4633]\n",
      "Wheghe onchit yadsingheriraveinckeer ncorather theathame s enowe arelallmbam pllicour mis w, ndalisthe \n",
      "\n",
      "[2m 8s (3000 30%) 2.4430]\n",
      "Whaterat anorses sin'TI:\n",
      "\n",
      "\n",
      "Fre.\n",
      "\n",
      "I we t:\n",
      "S:\n",
      "LES:\n",
      "\n",
      "Fow t thatrengon honouplt ss hene my Thencemanomy r, \n",
      "\n",
      "[2m 12s (3100 31%) 2.4926]\n",
      "Whe whe a beng thoner, are swe:\n",
      "UCat myoouth s hiere meas s, br wis tr t I'd, he th bunofond t\n",
      "werel h \n",
      "\n",
      "[2m 16s (3200 32%) 2.5026]\n",
      "Whest pou atelleris wee hangrel thois ve yo omy,\n",
      "Ahispere t burer wakindencengin she goured ime hemoth \n",
      "\n",
      "[2m 21s (3300 33%) 2.4519]\n",
      "Whethe I acrel; tin bupthatealed athe ave cast he thinininin anerende be,\n",
      "\n",
      "Pasheamake I be I w ay wed, \n",
      "\n",
      "[2m 26s (3400 34%) 2.4469]\n",
      "Whede meno dre whanehed the br ay tacoug therindorll inored GHit tithe ourerirgee\n",
      "\n",
      "I INGowaive f selid \n",
      "\n",
      "[2m 30s (3500 35%) 2.4428]\n",
      "Whianceoung worut cr thower wimunat\n",
      "ou, fore nt AREThe plenou te busthe t cthathers that the ses ms g, \n",
      "\n",
      "[2m 35s (3600 36%) 2.4444]\n",
      "Wh f titu se cedoustherst the d anovieat aset geroef I rd d!\n",
      "\n",
      "AMENG atoundas age o mere w meearanoomot \n",
      "\n",
      "[2m 39s (3700 37%) 2.5279]\n",
      "Wheat, and!\n",
      "THe?\n",
      "\n",
      "\n",
      "\n",
      "Ithaselpr ll d marg sthit I aned arals d he?\n",
      "MEie d, f tid s ar t ann w y heverord \n",
      "\n",
      "[2m 43s (3800 38%) 2.4787]\n",
      "Whed be ule\n",
      "Fil ve t ad ureameefowethen nveare m.\n",
      "\n",
      "R:\n",
      "He t t med t me owinghinoule he thin ule hel' me \n",
      "\n",
      "[2m 48s (3900 39%) 2.3988]\n",
      "Whougend shencourthath s han ce sinons mitil here ad hay.\n",
      "OUCoworougee eno th ck s VII tere:\n",
      "TENCUCICa \n",
      "\n",
      "[2m 52s (4000 40%) 2.4184]\n",
      "Whe e'llavereuis houist deaghe arerarthar:\n",
      "\n",
      "Pay nd botofitin fo my is wilinst ithel w s I:\n",
      "S:\n",
      "\n",
      "RD thie \n",
      "\n",
      "[2m 57s (4100 41%) 2.4355]\n",
      "Whe me lo t t wincheerds mmay ares htor towe w maugo math owamorinan hathow pounnde culandirgeasend s  \n",
      "\n",
      "[3m 1s (4200 42%) 2.4159]\n",
      "Whan'sthe urr peletlleds,\n",
      "\n",
      "An O:\n",
      "PENSABu l be ie.\n",
      "\n",
      "OFORDUS:\n",
      "Noure pestyod orardriea ay, onotorexf pt s \n",
      "\n",
      "[3m 5s (4300 43%) 2.4567]\n",
      "Whinst m:\n",
      "DY he:\n",
      "TINGROMomyo ive s thul f.\n",
      "\n",
      "We mer we be, ansavetaretethat ck ctino t ave pth.\n",
      "Sty tha \n",
      "\n",
      "[3m 9s (4400 44%) 2.5059]\n",
      "Wher m y, cono ind mat,\n",
      "IN mis wingan y ano ourutelatererinthor pemattharmo co me my nou lliter thicor \n",
      "\n",
      "[3m 14s (4500 45%) 2.4464]\n",
      "Whu tousththes:\n",
      "Theathe sweord Heay iner bur the lane ane as hond as ing om fe dste he sar t t ghear l \n",
      "\n",
      "[3m 18s (4600 46%) 2.4543]\n",
      "Wh an rd, ltithe dingiesous hand ithariropl'le cot, l t.\n",
      "I is thorke, she t wou y man s cisthalathee w \n",
      "\n",
      "[3m 22s (4700 47%) 2.4920]\n",
      "Wher bll wo,\n",
      "\n",
      "MI lil my; t an mar Wh y Mad hind thenfounine ithe fa ithand aze aimin theand, brsay, th \n",
      "\n",
      "[3m 26s (4800 48%) 2.4471]\n",
      "Whoum e cofe lorllisthomed ch th hre wifu.\n",
      "Whintthashinere henoreloveshe he d w, mance fo t t s mend w \n",
      "\n",
      "[3m 30s (4900 49%) 2.4879]\n",
      "When fr nge me ce ithagh simetinge thisait t ds my 'd p-ow be nd mes! bl hir?\n",
      "\n",
      "Bumaru thiniurst he ora \n",
      "\n",
      "[3m 35s (5000 50%) 2.4556]\n",
      "Whane ble herspendeng alyor hy, whexthe are bund ts h\n",
      "ATheromelan ader ondseders he myethe amewilegoor \n",
      "\n",
      "[3m 39s (5100 51%) 2.4837]\n",
      "Wheathelim,\n",
      "\n",
      "Sifrind ce ifonghethine be the rore thor tomee decorotere sher allis ave me f inot herd t \n",
      "\n",
      "[3m 44s (5200 52%) 2.4665]\n",
      "Whidel bullloriseve t, gee th fey y, Watu pr tr!\n",
      "Shend ithene mene inean th th athey y y\n",
      "KIn wigengufo \n",
      "\n",
      "[3m 49s (5300 53%) 2.4771]\n",
      "Wh ld no mutha f amfould pais ithe myoof the?\n",
      "LUMy hin min;\n",
      "Y ans atr heness be If tle henedithe at th \n",
      "\n",
      "[3m 53s (5400 54%) 2.4840]\n",
      "Wh athee ath ithe f ed thet gice thesthe fles a t l oy thart wis I ar myot; melly!\n",
      "d propime ureyothe  \n",
      "\n",
      "[3m 58s (5500 55%) 2.4517]\n",
      "Whty hater allay s,\n",
      "MIOun weellthinit hefearouseso heakitrd t mend, winthyouthoomomy e tet shotis I y  \n",
      "\n",
      "[4m 2s (5600 56%) 2.4808]\n",
      "Wh te sple w as yorlyo somes ofil s DWhangshoss t weswient me athind oror pe mise werath g berigan the \n",
      "\n",
      "[4m 6s (5700 56%) 2.4571]\n",
      "Whiva hetof outh fousthofild:\n",
      "YCind mat me s taulis rt t gh l VO:\n",
      "ANI howikllere imiougo hat bear spou \n",
      "\n",
      "[4m 11s (5800 57%) 2.4878]\n",
      "Whito t t ale see we hetrist w, bespelou orelomerot qunorealot byomisutrerdry mit g owathor ll th nour \n",
      "\n",
      "[4m 15s (5900 59%) 2.4030]\n",
      "Whe.\n",
      "A:\n",
      "Gaie s the ty hedireld aist ar.\n",
      "\n",
      "he ceseme co's y, whe be m ndate send n ur, t ou CIAng ithare \n",
      "\n",
      "[4m 19s (6000 60%) 2.4809]\n",
      "Whimend t Comay ame omyonestl rend brders th the the t goutwasthorenindouller t fore sho w t?\n",
      "Wh e ch  \n",
      "\n",
      "[4m 23s (6100 61%) 2.4876]\n",
      "Whir hes helf an apay thof me\n",
      "Prd thay,\n",
      "I or d t.\n",
      "Asbe I' u my gr himyou f is heare S:\n",
      "S:\n",
      "ARIORD thind \n",
      "\n",
      "[4m 27s (6200 62%) 2.4737]\n",
      "Whthe S:\n",
      "Find tou se t I sis alilis far w hedeawabat y teethis'd an, ishirof urs aphand the s thather  \n",
      "\n",
      "[4m 32s (6300 63%) 2.4769]\n",
      "Whaloulo p bous t char to aus ilfonse or thyod, coouptigor thichoreot yoy a tidshor ain t my s t of w  \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4m 37s (6400 64%) 2.4694]\n",
      "Whimy doonet, monitraniond.\n",
      "\n",
      "\n",
      "l ur.\n",
      "As ichadend thed stl GLAPle t ss th ll blt t, im.\n",
      "SThen ar he ane  \n",
      "\n",
      "[4m 41s (6500 65%) 2.4969]\n",
      "Whead wangist havear f s amare ag t iks ter athint the ce athacoure gengre y RDUTEThary\n",
      "NGot bup I st, \n",
      "\n",
      "[4m 46s (6600 66%) 2.4024]\n",
      "Whindon?\n",
      "ANGo yovererest hild my and bey couspr the.\n",
      "Lorsu asurore cande me t t anou the horear layoun \n",
      "\n",
      "[4m 50s (6700 67%) 2.4195]\n",
      "Whine I ouideleere til, t thoulisor ghinoutind hiveam hed han wind cite tha hifit ar lke me My.\n",
      "COfoth \n",
      "\n",
      "[4m 54s (6800 68%) 2.4191]\n",
      "Whind t's lls llanere Whthed hare beasthissene IAnt me tsthanks any l ate ghin s, lithe s oore y ditho \n",
      "\n",
      "[4m 58s (6900 69%) 2.4689]\n",
      "Whicerd t\n",
      "Win owhacarenome t pe hor towo or gid ikerco byours o h t w wen illave we we je the st w art \n",
      "\n",
      "[5m 3s (7000 70%) 2.4332]\n",
      "Whan ourd wray gak iellith IO:\n",
      "Now d then t, me oreasha whad arspefe ard, m t n ser.\n",
      "I ind wine me w w \n",
      "\n",
      "[5m 7s (7100 71%) 2.4234]\n",
      "Whe che u; rs wints's maix, thest then, aghe ewnd\n",
      "ARWhy d urd s s inthendeave m, al he, tr, on: he thi \n",
      "\n",
      "[5m 11s (7200 72%) 2.5129]\n",
      "Whit my ce talisethall wachathowowe, ners lorow,\n",
      "Panghede mes mons fan kenod wend the thor s gemour oc \n",
      "\n",
      "[5m 16s (7300 73%) 2.5096]\n",
      "Whatha fe wit ce winhalyof omano\n",
      "Le h by he-d sailis y m ar angan'sche, se the tend, mof\n",
      "BRI hay bang  \n",
      "\n",
      "[5m 20s (7400 74%) 2.5367]\n",
      "Whiveveray theat y, whiona t g wist d ge ous in fr heer ie mend t kngr f oureall go alins atil t cere  \n",
      "\n",
      "[5m 24s (7500 75%) 2.4904]\n",
      "Whing, hifanghearof drves polllinllof bea bld,\n",
      "WI aly trereseareathid onn ll best ave thous is t d le  \n",
      "\n",
      "[5m 29s (7600 76%) 2.5053]\n",
      "Wh thl me ink coonoud thamer atl wace m? we th fap mmamef mar bl there s o f\n",
      "AMar meathid hind.\n",
      "TAs si \n",
      "\n",
      "[5m 33s (7700 77%) 2.5101]\n",
      "Wh the por l.\n",
      "He ar coue ay t f y the' urkisereis llinduntw wene orindind maruthe, be teans torthencug \n",
      "\n",
      "[5m 37s (7800 78%) 2.4201]\n",
      "Whe seshitonger o wenthavearearthee fus win;\n",
      "\n",
      "En, thisulingus t maves gen s be n touse t osungerelle g \n",
      "\n",
      "[5m 41s (7900 79%) 2.4460]\n",
      "Whadsousomyont,\n",
      "OLIORI be he bot? t;\n",
      "\n",
      "'t ly he lllmume IOLAnton, e thethouthaglersthamontho wis I's, s \n",
      "\n",
      "[5m 46s (8000 80%) 2.4167]\n",
      "Wha d my thare t ll be se, d grd he ked LO we s sur t sthe:\n",
      "Thoowe Thy my c'th ce, grn\n",
      "\n",
      "Thare harese s \n",
      "\n",
      "[5m 51s (8100 81%) 2.4804]\n",
      "Whe s.\n",
      "LUpeng he chores, pathe s t n w\n",
      "Buresthathe I wig wase lle:\n",
      "HENoofrd tspoomurl ilty hive his dl \n",
      "\n",
      "[5m 55s (8200 82%) 2.4719]\n",
      "Whisat t pand.\n",
      "Theehy hathe med h w indin wimilinoutisticumin,\n",
      "Pull sthelive is sthe, as ld blithe my  \n",
      "\n",
      "[6m 0s (8300 83%) 2.5147]\n",
      "Who frave yofandeliscof ame t!\n",
      "I d s, sallenyend mo ittry, bo that he firand per f t tod dethe hangrer \n",
      "\n",
      "[6m 5s (8400 84%) 2.4509]\n",
      "Whe'sthe f the he.\n",
      "WAnd out ICAUThe ooure hato frat t m.\n",
      "To lotorve t this hegrixist this wicer.\n",
      "Andsi \n",
      "\n",
      "[6m 9s (8500 85%) 2.4607]\n",
      "When be oue, bed CHatomen, eloudesad theror m, hin t uelve k T:\n",
      "NCUSh os pr dst t be y the th han,\n",
      "RK: \n",
      "\n",
      "[6m 13s (8600 86%) 2.4428]\n",
      "Whas ca mpereand tefond thougeas anthet nouty me wivedis d Cithierte.\n",
      "Thirent BRIN:\n",
      "NE:\n",
      "INare tondere  \n",
      "\n",
      "[6m 18s (8700 87%) 2.4365]\n",
      "Wh ouspere.\n",
      "Arerur harathe ELI atite med, mid sht\n",
      "Tomy,\n",
      "KIn,\n",
      "Mand byowaporgheind:\n",
      "Anghe omee my tie m' \n",
      "\n",
      "[6m 22s (8800 88%) 2.4820]\n",
      "Whedis t comins to the puy, an be negr mele the poucullthe ave the f ar mumanat s foninin'd ter, yo no \n",
      "\n",
      "[6m 27s (8900 89%) 2.5084]\n",
      "Whe t ll y hal s wncad y nolang-wnkil our thize ly me towathiry le tol allly mnghime Is allititendis f \n",
      "\n",
      "[6m 33s (9000 90%) 2.4434]\n",
      "Wheay asthanered co bbusty Y a fothe th t t thous angrou th pe se t s wor s tond wioure.\n",
      "\n",
      "Toutatal t t \n",
      "\n",
      "[6m 37s (9100 91%) 2.4897]\n",
      "Whe, theraine.\n",
      "CHeces is, wis?\n",
      "POLE mancederdow, thins, ICANo l seatined thef s KIN h anomanie tisthen \n",
      "\n",
      "[6m 42s (9200 92%) 2.4494]\n",
      "Whoulis\n",
      "OLUER:\n",
      "Whoue th ispove sthikyorelan l qude harans s te s, onra mue fo he thavennd hang, a and  \n",
      "\n",
      "[6m 46s (9300 93%) 2.5047]\n",
      "Whimy:\n",
      "F mu he, atsen t s INI withe ave t'ds boourean thark oang wn ir bessu incathe d, nerot itofetel \n",
      "\n",
      "[6m 51s (9400 94%) 2.4868]\n",
      "Wheril gos beaves f s t be anghe wie RI chowico menceacou ses wimene f, thimbalinde me thoury t cand s \n",
      "\n",
      "[6m 55s (9500 95%) 2.4569]\n",
      "Whakner mainond ally herin abeappt fr herst olugarthint filofr o morote pr athe, to boulsis tot t f s  \n",
      "\n",
      "[7m 0s (9600 96%) 2.4747]\n",
      "Wherbe at coun rnd flarith t n foufelld t te wer ando-lllle't ashagheader heshautheveve t, tive her EL \n",
      "\n",
      "[7m 4s (9700 97%) 2.4571]\n",
      "Whe thans t a hin figeld oumancer GRUSiseeeavagears.\n",
      "Bes handy atho yo nt lyoll KEswimendoonor nd ns h \n",
      "\n",
      "[7m 8s (9800 98%) 2.4386]\n",
      "Whin hee'dithaverd ind istrs myomare s;\n",
      "\n",
      "ANGORIZAnd avince iredource.\n",
      "\n",
      "Thee d t omeld sererer kerang h \n",
      "\n",
      "[7m 12s (9900 99%) 2.4304]\n",
      "Whely onde ck ce rdo nghe aitherim bathat f w, akeis nituthis g as he morout th isimourks.\n",
      "NGo se\n",
      "CE:\n",
      " \n",
      "\n",
      "[7m 17s (10000 100%) 2.4693]\n",
      "Whyo ies asadetheane s wit chooon oroves wout ave higbarane!\n",
      "spallourer g yout thirs My, atas athe thu \n",
      "\n"
     ]
    }
   ],
   "source": [
    "###Parameters\n",
    "n_epochs = 10000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 5\n",
    "lr = 0.005\n",
    "batch_size = 16\n",
    "chunk_len = 80\n",
    "\n",
    "model3 = RNN(n_characters, hidden_size, n_characters, n_layers, nn.LSTM) #create model\n",
    "model_optimizer3 = torch.optim.Adam(model3.parameters(), lr=lr) #create Adam optimizer\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() #chose criterion\n",
    "\n",
    "start = time.time()\n",
    "all_losses3 = []\n",
    "loss_avg = 0\n",
    "\n",
    "def train(inp, target):\n",
    "    \"\"\"\n",
    "    Train sequence for one chunk:\n",
    "    \"\"\"\n",
    "    #reset gradients\n",
    "    model_optimizer3.zero_grad() \n",
    "    \n",
    "    # predict output\n",
    "    output = model3(inp)\n",
    "    \n",
    "    #compute loss\n",
    "    loss =  criterion(output.view(batch_size*chunk_len,-1), target.view(-1)) \n",
    "\n",
    "    #compute gradients and backpropagate\n",
    "    loss.backward() \n",
    "    model_optimizer3.step() \n",
    "\n",
    "    return loss.data.item() \n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set(chunk_len,batch_size))  #train on one chunk \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(generate(model3,'Wh', 100), '\\n')\n",
    "       \n",
    "\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses3.append(loss_avg / plot_every)\n",
    "        loss_avg = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize loss \n",
    "\n",
    "**RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x146b36586c8>]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x146ac86ef08>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x146ac88c1c8>]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(all_losses3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try different temperatures\n",
    "\n",
    "Changing the distribution sharpness has an impact on character sampling:\n",
    "\n",
    "more or less probable things are sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\William\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1698: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THES:\n",
      "DUCHon ng.\n",
      "D:\n",
      "D y\n",
      "Chy, by w Gr.\n",
      "otho; at ourilard\n",
      "AMI cow my, ar ashy, INofitin;\n",
      "Daigr wh othe ck t il akllanerire s, k y mair\n",
      "LARO:\n",
      "Thinie, sea p,\n",
      "Ans cr tsoot'sthithe the pud.\n",
      "VI ak'?\n",
      "\n",
      "\n",
      "GBu oun\n",
      "----\n",
      "Thon whem, methe:\n",
      "O:\n",
      "Ashe wh ckers, belemy he othoites\n",
      "\n",
      "CANIOROFr d tunougir t,\n",
      "Seathithen; me ad\n",
      "IUSTheathashe s thimatose wanomewaif al th be hes wome u I houthal kis,\n",
      "BUCAne,\n",
      "To hea tirpust d waf vom\n",
      "----\n",
      "Thind w het s y gouth s thishare hend wis then wesen s car ouris ake f thit igre anthe t he me we wisee my m the ane he chen hale IZARO:\n",
      "The ithean ha be se har t the s morr an there whe whe be the mise\n",
      "----\n",
      "The the thes he the histhe the the the be mathin the s ane ll hen he anger he he the be hit ss w an he the he we wis athathe the han he ang cor he the he the s t he here sthe he win wathand the areanore\n",
      "----\n",
      "The he the he the the t the the the the an he the the he he the the the he the the the he he the he he the he the he the the the the the the the the the the the he he the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "print(generate(model,'T', 200, temperature=1))\n",
    "print(\"----\")\n",
    "print(generate(model,'Th', 200, temperature=0.8))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.5))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.3))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model,'Th', 200, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GRU**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That trorencr ia ure thamisthe s therir bu, gher,\n",
      "ser, hy.\n",
      "I had ANUSwingothathoy r grt t k, hts te hiantas hinse.\n",
      "LEN m, llest mere hanf anofinofor PUCETh, ws serssure Dof og! ll, be he achise he thee\n",
      "----\n",
      "Thak ke Fo my ten ond s,\n",
      "SSe, ane sped sour d ucath thest hakelld Hound alo fthe, y he hast trou nderorcowan Bul nk me as thaticouthat tine lyers fo bicht at bave tiliswoour gr he s,\n",
      "Harth thaithoobr me\n",
      "----\n",
      "Th t t the the at ther he thour mour s s sth wie t my me olome mou the yotad le t sare d t the me t te sthow are s t t thar the s w s an the thee s st m be acl he be mord lo beathe the I the se t are ll\n",
      "----\n",
      "The he outhathe our the than s the thare s s the win anore s t th the y atour thes ar me t the me boure or me the t that the t t tho here me s the whar t me the s y t thathe s he t se s the the me s s a\n",
      "----\n",
      "The the the the the the the the the the the the the the the the the t the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the \n"
     ]
    }
   ],
   "source": [
    "print(generate(model2,'T', 200, temperature=1))\n",
    "print(\"----\")\n",
    "print(generate(model2,'Th', 200, temperature=0.8))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model2,'Th', 200, temperature=0.5))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model2,'Th', 200, temperature=0.3))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model2,'Th', 200, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teasore r g ke-lowrrandser lldueer s y IUS:\n",
      "MERIs lor hiver tou dofatheade.\n",
      "Fis so bre atholamis aronsheres, a p owivit th\n",
      "\n",
      "FI g, t.\n",
      "Ther artoveavoklashandong st math hthad r ts t,---\n",
      "Wanitred thathath\n",
      "----\n",
      "Th astal ayouks, athe toust t he we as bl aw, d mes, iser tstator s m ir y maly,-be ithig y thoncall rssow br wais f y bll nere teathe istuthuemume ithathod u ined inere fou y wom ouly ncee ans y; d tho\n",
      "----\n",
      "The wo d n ther an aran is t y as we d than and I thou at thashe s nd llll s f p y l s pour tor ithe\n",
      "Wat thasther t as s tous ad fand akerr t the as s st t hithe by le, the shinoure y ar he athe s ben t\n",
      "----\n",
      "The arere and s wis way thand th ce the and athe the the the fe t it the the the s the an thare the the whe theathe the be my s the he thare the are se the thather at he t the than the t whe thathe t th\n",
      "----\n",
      "The the the the the the the athe the the the the the the t the the the the the the the the the t the the the the athe and the the the the than the the the the the the the the the the the the the the the\n"
     ]
    }
   ],
   "source": [
    "print(generate(model3,'T', 200, temperature=1))\n",
    "print(\"----\")\n",
    "print(generate(model3,'Th', 200, temperature=0.8))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model3,'Th', 200, temperature=0.5))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model3,'Th', 200, temperature=0.3))\n",
    "print(\"----\")\n",
    "\n",
    "print(generate(model3,'Th', 200, temperature=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving this code:\n",
    "\n",
    "(a) Tinker with parameters:\n",
    "\n",
    "- Is it really necessary to have 100 dims character embeddings\n",
    "- Chunk length can be gradually increased\n",
    "- Try changing RNN cell type (GRUs - LSTMs)\n",
    "\n",
    "(b) Add GPU support to go faster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------ End of practical\n",
    "\n",
    "#### Legacy loading code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!', 1)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from os.path import split as pathsplit\n",
    "\n",
    "# dir_train = \"dataset/aclImdb/train/\"\n",
    "# dir_test = \"dataset/aclImdb/test/\"\n",
    "\n",
    "train_files = glob.glob(dir_train+'pos/*.txt') + glob.glob(dir_train+'neg/*.txt')\n",
    "test_files = glob.glob(dir_test+'pos/*.txt') + glob.glob(dir_test+'neg/*.txt')\n",
    "\n",
    "def get_polarity(f):\n",
    "    \"\"\"\n",
    "    Extracts polarity from filename:\n",
    "    0 is negative (< 5)\n",
    "    1 is positive (> 5)\n",
    "    \"\"\"\n",
    "    _,name = pathsplit(f)\n",
    "    if int(name.split('_')[1].split('.')[0]) < 5:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "\n",
    "def open_one(f):\n",
    "    \n",
    "    polarity = get_polarity(f)\n",
    "    \n",
    "    with open(f,\"r\", encoding=\"utf8\") as review:\n",
    "        text = \" \".join(review.readlines()).strip()\n",
    "    \n",
    "    return (text,polarity)\n",
    "\n",
    "print(open_one(train_files[0]))\n",
    "\n",
    "train = [open_one(x) for x in train_files] #contains (text,pol) couples\n",
    "test = [open_one(x) for x in test_files]   #contains (text,pol) couples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
